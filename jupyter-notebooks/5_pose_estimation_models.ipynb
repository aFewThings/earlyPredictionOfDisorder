{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimation Models\n",
    "\n",
    "references:\n",
    "- stacked hourglass networks for human pose estimation: [https://arxiv.org/abs/1603.06937](https://arxiv.org/abs/1603.06937)\n",
    "- simple baselines for human pose estimation and tracking: [https://arxiv.org/abs/1804.06208](https://arxiv.org/abs/1804.06208)\n",
    "- [https://curt-park.github.io/2018-07-03/stacked-hourglass-networks-for-human-pose-estimation/](https://curt-park.github.io/2018-07-03/stacked-hourglass-networks-for-human-pose-estimation/)\n",
    "- Deconvolution(Transposed Convolution) [https://zzsza.github.io/data/2018/06/25/upsampling-with-transposed-convolution/)\n",
    "\n",
    "code references: \n",
    "- [https://github.com/bearpaw/pytorch-pose](https://github.com/bearpaw/pytorch-pose)\n",
    "- [https://github.com/princeton-vl/pose-hg-train](https://github.com/princeton-vl/pose-hg-train)\n",
    "- [https://github.com/microsoft/human-pose-estimation.pytorch](https://github.com/microsoft/human-pose-estimation.pytorch)\n",
    "\n",
    "dataset annotatation references:\n",
    "- [https://github.com/bearpaw/pytorch-pose](https://github.com/bearpaw/pytorch-pose)\n",
    "- [https://github.com/HRNet/HRNet-Human-Pose-Estimation](https://github.com/HRNet/HRNet-Human-Pose-Estimation)\n",
    "\n",
    "### Stacked Hourglass Networks for Human Pose Estimation (2016)\n",
    "\n",
    "#### Multi-Stage Architecture\n",
    "\n",
    "<img src=\"./src_imgs/10.png\" width=600><br/>\n",
    "\n",
    "<img src=\"./src_imgs/9.jpg\"><br/>\n",
    "\n",
    "\"stacked hourglass networks for human pose estimation\" 논문에서 제시한 Stacked Hourglass는 말그대로 모래시계 모양의 동일한 Network를 여러겹 쌓아서 만든 모델입니다. 이렇게 단일 네트워크를 여러겹 쌓는 구조는 Pose Estimation 분야에서 종종 볼 수 있습니다. 그 이유는 각 Stack마다 heatmap을 출력해보면 알 수 있는데, 2번째 그림과 같이 결과를 refine 시킬 수 있다는 장점을 갖기 때문입니다. 이러한 구조를 Multi-Stage Architecture라 하며 Single-Stage Architecture보다 정밀한 결과를 얻을 것이라는 아이디어에서 출발하였습니다. \n",
    "\n",
    "#### Multi-Scale\n",
    "\n",
    "|![](./src_imgs/6.png)|\n",
    "|:---:|\n",
    "|*Hourglass*|\n",
    "\n",
    "|![](./src_imgs/7.png)|\n",
    "|:---:|\n",
    "|*Stacked Hourglass (two-stack)*|\n",
    "\n",
    "Stacked Hourglass가 포즈를 추출하는 것에 있어 우수한 성능을 내는 또 하나의 이유는 이미지의 모든 scale에서 정보를 얻을 수 있기 때문입니다. 단일 Hourglass network는 high resolution에서 low resolution으로 features를 만들어내는 **Downsampling Process**, 다시 low resolution에서 high resolution으로 resolution을 복구시키는 **Upsampling Process**를 통해 다양한 scale에서 정보를 얻습니다 (e.g. 64->32->16->8->4, 4->8->16->32->64). \n",
    "\n",
    "#### Residual Block / Intermediate Supervision\n",
    "\n",
    "|![](./src_imgs/11.png)|\n",
    "|:---:|\n",
    "|*Residual Block*|\n",
    "\n",
    "|![](./src_imgs/12.png)|\n",
    "|:---:|\n",
    "|*Intermediate Supervision*|\n",
    "\n",
    "Multi-Stage 구조의 가장 큰 문제는 네트워크가 쌓일수록 layer가 깊어져서 Deep Neural Net의 고질적인 문제인 vanishing gradient(깊은 layer에서는 gradient의 영향력이 작아지는 문제)와 degradation(얕은 layer보다 깊은 layer를 갖는 모델이 오히려 성능이 안좋아지는 문제)이 발생한다는 점입니다. \n",
    "이것을 Stacked Hourglass는 residual block과 intermediate supervision을 차용함으로 학습 문제를 개선하였습니다.\n",
    "\n",
    "Residual block의 특징은 module의 input과 output(convolution + activation function 결과)을 element-wise addition한다는 것입니다. 이러한 구조는 gradient를 유지하기 쉽게 만들어주기 때문에 vanishing gradient을 개선할 수 있는 장점이 존재합니다.\n",
    "\n",
    "Intermediate supervision이란 각 stack 끝마다 loss layer를 추가하는 것으로 gradient를 비효율적이지만 효과적으로 전달하는 방법입니다. 깊은 stack일수록 gradient의 값이 0에 가까우므로 gradient를 깊은 레이어까지 직접 전달하는 것으로 해결하였습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 정의 및 구조 시각화\n",
    "\n",
    "이제 Hourglass 모델을 정의하고 stack=2 만큼 쌓은 네트워크의 구조를 시각화하겠습니다.\n",
    "\n",
    "Hourglass 모델을 이루는 기본 block은 앞서 말했듯이 Residual block을 사용합니다.<br/>\n",
    "Block의 입력인 x와 컨볼루션 결과인 out이 함께 더해져서 연결되는 모습(skip connection)을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# residual block\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 정의한 residual block은 다음 hourglass module에서 기본 block으로 사용됩니다. \n",
    "\n",
    "hourglass에선 다양한 scale의 resolution에서 정보를 얻기 위해 MaxPooling을 이용해 downsampling 하고, 최근접 이웃보간을 통해 upsampling 합니다 (`F.max_pool2d()`, `F.interpolate()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hourglass(nn.Module):\n",
    "    def __init__(self, block, num_blocks, planes, depth):\n",
    "        super(Hourglass, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.block = block\n",
    "        self.hg = self._make_hour_glass(block, num_blocks, planes, depth)\n",
    "\n",
    "    def _make_residual(self, block, num_blocks, planes):\n",
    "        layers = []\n",
    "        for i in range(0, num_blocks):\n",
    "            layers.append(block(planes*block.expansion, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_hour_glass(self, block, num_blocks, planes, depth):\n",
    "        hg = []\n",
    "        for i in range(depth):\n",
    "            res = []\n",
    "            for j in range(3):\n",
    "                res.append(self._make_residual(block, num_blocks, planes))\n",
    "            if i == 0:\n",
    "                res.append(self._make_residual(block, num_blocks, planes))\n",
    "            hg.append(nn.ModuleList(res))\n",
    "        return nn.ModuleList(hg)\n",
    "\n",
    "    def _hour_glass_forward(self, n, x):\n",
    "        up1 = self.hg[n-1][0](x)\n",
    "        low1 = F.max_pool2d(x, 2, stride=2)\n",
    "        low1 = self.hg[n-1][1](low1)\n",
    "\n",
    "        if n > 1:\n",
    "            low2 = self._hour_glass_forward(n-1, low1)\n",
    "        else:\n",
    "            low2 = self.hg[n-1][3](low1)\n",
    "        low3 = self.hg[n-1][2](low2)\n",
    "        up2 = F.interpolate(low3, scale_factor=2) # Nearest Neighbour\n",
    "        out = up1 + up2\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._hour_glass_forward(self.depth, x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Hourglass 모듈을 stack으로 쌓을 수 있는 네트워크를 정의합니다.\n",
    "\n",
    "`HourglassNet`은 hourglass 스택 개수(`num_stack`), 한 block에 포함된 residual block 개수(`num_block`), parts 개수(`num_classes`)를 조정함으로 모델의 크기를 정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " class HourglassNet(nn.Module):\n",
    "    '''Hourglass model from Newell et al ECCV 2016'''\n",
    "    def __init__(self, block, num_stacks=2, num_blocks=4, num_classes=16):\n",
    "        super(HourglassNet, self).__init__()\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.num_feats = 128\n",
    "        self.num_stacks = num_stacks\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_residual(block, self.inplanes, 1)\n",
    "        self.layer2 = self._make_residual(block, self.inplanes, 1)\n",
    "        self.layer3 = self._make_residual(block, self.num_feats, 1)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        # build hourglass modules\n",
    "        ch = self.num_feats*block.expansion\n",
    "        hg, res, fc, score, fc_, score_ = [], [], [], [], [], []\n",
    "        for i in range(num_stacks):\n",
    "            hg.append(Hourglass(block, num_blocks, self.num_feats, 4))\n",
    "            res.append(self._make_residual(block, self.num_feats, num_blocks))\n",
    "            fc.append(self._make_fc(ch, ch))\n",
    "            score.append(nn.Conv2d(ch, num_classes, kernel_size=1, bias=True))\n",
    "            if i < num_stacks-1:\n",
    "                fc_.append(nn.Conv2d(ch, ch, kernel_size=1, bias=True))\n",
    "                score_.append(nn.Conv2d(num_classes, ch, kernel_size=1, bias=True))\n",
    "        self.hg = nn.ModuleList(hg)\n",
    "        self.res = nn.ModuleList(res)\n",
    "        self.fc = nn.ModuleList(fc)\n",
    "        self.score = nn.ModuleList(score)\n",
    "        self.fc_ = nn.ModuleList(fc_)\n",
    "        self.score_ = nn.ModuleList(score_)\n",
    "\n",
    "    def _make_residual(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=True),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_fc(self, inplanes, outplanes):\n",
    "        bn = nn.BatchNorm2d(inplanes)\n",
    "        conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=True)\n",
    "        return nn.Sequential(\n",
    "                conv,\n",
    "                bn,\n",
    "                self.relu,\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        # preprocessing\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        # stacking modules\n",
    "        for i in range(self.num_stacks):\n",
    "            y = self.hg[i](x)\n",
    "            y = self.res[i](y)\n",
    "            y = self.fc[i](y)\n",
    "            score = self.score[i](y)\n",
    "            out.append(score)\n",
    "            if i < self.num_stacks-1:\n",
    "                fc_ = self.fc_[i](y)\n",
    "                score_ = self.score_[i](score)\n",
    "                x = x + fc_ + score_\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack=1인 HourglassNet의 구조를 출력해보면 다음과 같습니다. \n",
    "\n",
    "레이어가 깊어질수록 output resolution이 128->64->32->16->8->4 까지 줄어들고, \n",
    "다시 4->8->16->32->64 까지 증가함을 볼 수 있습니다. 또한, 마지막 레이어에선 heatmap([1, 16, 64, 64])을 출력하고 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 64, 128, 128]           9,472\n",
      "       BatchNorm2d-2          [1, 64, 128, 128]             128\n",
      "              ReLU-3          [1, 64, 128, 128]               0\n",
      "              ReLU-4          [1, 64, 128, 128]               0\n",
      "       BatchNorm2d-5          [1, 64, 128, 128]             128\n",
      "              ReLU-6          [1, 64, 128, 128]               0\n",
      "            Conv2d-7          [1, 64, 128, 128]           4,160\n",
      "       BatchNorm2d-8          [1, 64, 128, 128]             128\n",
      "              ReLU-9          [1, 64, 128, 128]               0\n",
      "           Conv2d-10          [1, 64, 128, 128]          36,928\n",
      "      BatchNorm2d-11          [1, 64, 128, 128]             128\n",
      "             ReLU-12          [1, 64, 128, 128]               0\n",
      "           Conv2d-13         [1, 128, 128, 128]           8,320\n",
      "           Conv2d-14         [1, 128, 128, 128]           8,320\n",
      "       Bottleneck-15         [1, 128, 128, 128]               0\n",
      "        MaxPool2d-16           [1, 128, 64, 64]               0\n",
      "      BatchNorm2d-17           [1, 128, 64, 64]             256\n",
      "             ReLU-18           [1, 128, 64, 64]               0\n",
      "           Conv2d-19           [1, 128, 64, 64]          16,512\n",
      "      BatchNorm2d-20           [1, 128, 64, 64]             256\n",
      "             ReLU-21           [1, 128, 64, 64]               0\n",
      "           Conv2d-22           [1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-23           [1, 128, 64, 64]             256\n",
      "             ReLU-24           [1, 128, 64, 64]               0\n",
      "           Conv2d-25           [1, 256, 64, 64]          33,024\n",
      "           Conv2d-26           [1, 256, 64, 64]          33,024\n",
      "       Bottleneck-27           [1, 256, 64, 64]               0\n",
      "      BatchNorm2d-28           [1, 256, 64, 64]             512\n",
      "             ReLU-29           [1, 256, 64, 64]               0\n",
      "           Conv2d-30           [1, 128, 64, 64]          32,896\n",
      "      BatchNorm2d-31           [1, 128, 64, 64]             256\n",
      "             ReLU-32           [1, 128, 64, 64]               0\n",
      "           Conv2d-33           [1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-34           [1, 128, 64, 64]             256\n",
      "             ReLU-35           [1, 128, 64, 64]               0\n",
      "           Conv2d-36           [1, 256, 64, 64]          33,024\n",
      "       Bottleneck-37           [1, 256, 64, 64]               0\n",
      "      BatchNorm2d-38           [1, 256, 64, 64]             512\n",
      "             ReLU-39           [1, 256, 64, 64]               0\n",
      "           Conv2d-40           [1, 128, 64, 64]          32,896\n",
      "      BatchNorm2d-41           [1, 128, 64, 64]             256\n",
      "             ReLU-42           [1, 128, 64, 64]               0\n",
      "           Conv2d-43           [1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-44           [1, 128, 64, 64]             256\n",
      "             ReLU-45           [1, 128, 64, 64]               0\n",
      "           Conv2d-46           [1, 256, 64, 64]          33,024\n",
      "       Bottleneck-47           [1, 256, 64, 64]               0\n",
      "      BatchNorm2d-48           [1, 256, 32, 32]             512\n",
      "             ReLU-49           [1, 256, 32, 32]               0\n",
      "           Conv2d-50           [1, 128, 32, 32]          32,896\n",
      "      BatchNorm2d-51           [1, 128, 32, 32]             256\n",
      "             ReLU-52           [1, 128, 32, 32]               0\n",
      "           Conv2d-53           [1, 128, 32, 32]         147,584\n",
      "      BatchNorm2d-54           [1, 128, 32, 32]             256\n",
      "             ReLU-55           [1, 128, 32, 32]               0\n",
      "           Conv2d-56           [1, 256, 32, 32]          33,024\n",
      "       Bottleneck-57           [1, 256, 32, 32]               0\n",
      "      BatchNorm2d-58           [1, 256, 32, 32]             512\n",
      "             ReLU-59           [1, 256, 32, 32]               0\n",
      "           Conv2d-60           [1, 128, 32, 32]          32,896\n",
      "      BatchNorm2d-61           [1, 128, 32, 32]             256\n",
      "             ReLU-62           [1, 128, 32, 32]               0\n",
      "           Conv2d-63           [1, 128, 32, 32]         147,584\n",
      "      BatchNorm2d-64           [1, 128, 32, 32]             256\n",
      "             ReLU-65           [1, 128, 32, 32]               0\n",
      "           Conv2d-66           [1, 256, 32, 32]          33,024\n",
      "       Bottleneck-67           [1, 256, 32, 32]               0\n",
      "      BatchNorm2d-68           [1, 256, 16, 16]             512\n",
      "             ReLU-69           [1, 256, 16, 16]               0\n",
      "           Conv2d-70           [1, 128, 16, 16]          32,896\n",
      "      BatchNorm2d-71           [1, 128, 16, 16]             256\n",
      "             ReLU-72           [1, 128, 16, 16]               0\n",
      "           Conv2d-73           [1, 128, 16, 16]         147,584\n",
      "      BatchNorm2d-74           [1, 128, 16, 16]             256\n",
      "             ReLU-75           [1, 128, 16, 16]               0\n",
      "           Conv2d-76           [1, 256, 16, 16]          33,024\n",
      "       Bottleneck-77           [1, 256, 16, 16]               0\n",
      "      BatchNorm2d-78           [1, 256, 16, 16]             512\n",
      "             ReLU-79           [1, 256, 16, 16]               0\n",
      "           Conv2d-80           [1, 128, 16, 16]          32,896\n",
      "      BatchNorm2d-81           [1, 128, 16, 16]             256\n",
      "             ReLU-82           [1, 128, 16, 16]               0\n",
      "           Conv2d-83           [1, 128, 16, 16]         147,584\n",
      "      BatchNorm2d-84           [1, 128, 16, 16]             256\n",
      "             ReLU-85           [1, 128, 16, 16]               0\n",
      "           Conv2d-86           [1, 256, 16, 16]          33,024\n",
      "       Bottleneck-87           [1, 256, 16, 16]               0\n",
      "      BatchNorm2d-88             [1, 256, 8, 8]             512\n",
      "             ReLU-89             [1, 256, 8, 8]               0\n",
      "           Conv2d-90             [1, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-91             [1, 128, 8, 8]             256\n",
      "             ReLU-92             [1, 128, 8, 8]               0\n",
      "           Conv2d-93             [1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-94             [1, 128, 8, 8]             256\n",
      "             ReLU-95             [1, 128, 8, 8]               0\n",
      "           Conv2d-96             [1, 256, 8, 8]          33,024\n",
      "       Bottleneck-97             [1, 256, 8, 8]               0\n",
      "      BatchNorm2d-98             [1, 256, 8, 8]             512\n",
      "             ReLU-99             [1, 256, 8, 8]               0\n",
      "          Conv2d-100             [1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-101             [1, 128, 8, 8]             256\n",
      "            ReLU-102             [1, 128, 8, 8]               0\n",
      "          Conv2d-103             [1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-104             [1, 128, 8, 8]             256\n",
      "            ReLU-105             [1, 128, 8, 8]               0\n",
      "          Conv2d-106             [1, 256, 8, 8]          33,024\n",
      "      Bottleneck-107             [1, 256, 8, 8]               0\n",
      "     BatchNorm2d-108             [1, 256, 4, 4]             512\n",
      "            ReLU-109             [1, 256, 4, 4]               0\n",
      "          Conv2d-110             [1, 128, 4, 4]          32,896\n",
      "     BatchNorm2d-111             [1, 128, 4, 4]             256\n",
      "            ReLU-112             [1, 128, 4, 4]               0\n",
      "          Conv2d-113             [1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-114             [1, 128, 4, 4]             256\n",
      "            ReLU-115             [1, 128, 4, 4]               0\n",
      "          Conv2d-116             [1, 256, 4, 4]          33,024\n",
      "      Bottleneck-117             [1, 256, 4, 4]               0\n",
      "     BatchNorm2d-118             [1, 256, 4, 4]             512\n",
      "            ReLU-119             [1, 256, 4, 4]               0\n",
      "          Conv2d-120             [1, 128, 4, 4]          32,896\n",
      "     BatchNorm2d-121             [1, 128, 4, 4]             256\n",
      "            ReLU-122             [1, 128, 4, 4]               0\n",
      "          Conv2d-123             [1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-124             [1, 128, 4, 4]             256\n",
      "            ReLU-125             [1, 128, 4, 4]               0\n",
      "          Conv2d-126             [1, 256, 4, 4]          33,024\n",
      "      Bottleneck-127             [1, 256, 4, 4]               0\n",
      "     BatchNorm2d-128             [1, 256, 4, 4]             512\n",
      "            ReLU-129             [1, 256, 4, 4]               0\n",
      "          Conv2d-130             [1, 128, 4, 4]          32,896\n",
      "     BatchNorm2d-131             [1, 128, 4, 4]             256\n",
      "            ReLU-132             [1, 128, 4, 4]               0\n",
      "          Conv2d-133             [1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-134             [1, 128, 4, 4]             256\n",
      "            ReLU-135             [1, 128, 4, 4]               0\n",
      "          Conv2d-136             [1, 256, 4, 4]          33,024\n",
      "      Bottleneck-137             [1, 256, 4, 4]               0\n",
      "     BatchNorm2d-138             [1, 256, 8, 8]             512\n",
      "            ReLU-139             [1, 256, 8, 8]               0\n",
      "          Conv2d-140             [1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-141             [1, 128, 8, 8]             256\n",
      "            ReLU-142             [1, 128, 8, 8]               0\n",
      "          Conv2d-143             [1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-144             [1, 128, 8, 8]             256\n",
      "            ReLU-145             [1, 128, 8, 8]               0\n",
      "          Conv2d-146             [1, 256, 8, 8]          33,024\n",
      "      Bottleneck-147             [1, 256, 8, 8]               0\n",
      "     BatchNorm2d-148           [1, 256, 16, 16]             512\n",
      "            ReLU-149           [1, 256, 16, 16]               0\n",
      "          Conv2d-150           [1, 128, 16, 16]          32,896\n",
      "     BatchNorm2d-151           [1, 128, 16, 16]             256\n",
      "            ReLU-152           [1, 128, 16, 16]               0\n",
      "          Conv2d-153           [1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-154           [1, 128, 16, 16]             256\n",
      "            ReLU-155           [1, 128, 16, 16]               0\n",
      "          Conv2d-156           [1, 256, 16, 16]          33,024\n",
      "      Bottleneck-157           [1, 256, 16, 16]               0\n",
      "     BatchNorm2d-158           [1, 256, 32, 32]             512\n",
      "            ReLU-159           [1, 256, 32, 32]               0\n",
      "          Conv2d-160           [1, 128, 32, 32]          32,896\n",
      "     BatchNorm2d-161           [1, 128, 32, 32]             256\n",
      "            ReLU-162           [1, 128, 32, 32]               0\n",
      "          Conv2d-163           [1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-164           [1, 128, 32, 32]             256\n",
      "            ReLU-165           [1, 128, 32, 32]               0\n",
      "          Conv2d-166           [1, 256, 32, 32]          33,024\n",
      "      Bottleneck-167           [1, 256, 32, 32]               0\n",
      "       Hourglass-168           [1, 256, 64, 64]               0\n",
      "     BatchNorm2d-169           [1, 256, 64, 64]             512\n",
      "            ReLU-170           [1, 256, 64, 64]               0\n",
      "          Conv2d-171           [1, 128, 64, 64]          32,896\n",
      "     BatchNorm2d-172           [1, 128, 64, 64]             256\n",
      "            ReLU-173           [1, 128, 64, 64]               0\n",
      "          Conv2d-174           [1, 128, 64, 64]         147,584\n",
      "     BatchNorm2d-175           [1, 128, 64, 64]             256\n",
      "            ReLU-176           [1, 128, 64, 64]               0\n",
      "          Conv2d-177           [1, 256, 64, 64]          33,024\n",
      "      Bottleneck-178           [1, 256, 64, 64]               0\n",
      "          Conv2d-179           [1, 256, 64, 64]          65,792\n",
      "     BatchNorm2d-180           [1, 256, 64, 64]             512\n",
      "            ReLU-181           [1, 256, 64, 64]               0\n",
      "            ReLU-182           [1, 256, 64, 64]               0\n",
      "          Conv2d-183            [1, 16, 64, 64]           4,112\n",
      "================================================================\n",
      "Total params: 3,586,960\n",
      "Trainable params: 3,586,960\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 468.28\n",
      "Params size (MB): 13.68\n",
      "Estimated Total Size (MB): 482.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# hourglass\n",
    "model = HourglassNet(Bottleneck, num_stacks=1, num_blocks=1, num_classes=16).to(device)\n",
    "\n",
    "summary(model, input_size=(3, 256, 256), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 64, 128, 128]           9,472\n",
      "       BatchNorm2d-2          [1, 64, 128, 128]             128\n",
      "              ReLU-3          [1, 64, 128, 128]               0\n",
      "              ReLU-4          [1, 64, 128, 128]               0\n",
      "              ReLU-5          [1, 64, 128, 128]               0\n",
      "       BatchNorm2d-6          [1, 64, 128, 128]             128\n",
      "              ReLU-7          [1, 64, 128, 128]               0\n",
      "            Conv2d-8          [1, 64, 128, 128]           4,160\n",
      "       BatchNorm2d-9          [1, 64, 128, 128]             128\n",
      "             ReLU-10          [1, 64, 128, 128]               0\n",
      "           Conv2d-11          [1, 64, 128, 128]          36,928\n",
      "      BatchNorm2d-12          [1, 64, 128, 128]             128\n",
      "             ReLU-13          [1, 64, 128, 128]               0\n",
      "           Conv2d-14         [1, 128, 128, 128]           8,320\n",
      "           Conv2d-15         [1, 128, 128, 128]           8,320\n",
      "       Bottleneck-16         [1, 128, 128, 128]               0\n",
      "        MaxPool2d-17           [1, 128, 64, 64]               0\n",
      "      BatchNorm2d-18           [1, 128, 64, 64]             256\n",
      "             ReLU-19           [1, 128, 64, 64]               0\n",
      "           Conv2d-20           [1, 128, 64, 64]          16,512\n",
      "      BatchNorm2d-21           [1, 128, 64, 64]             256\n",
      "             ReLU-22           [1, 128, 64, 64]               0\n",
      "           Conv2d-23           [1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-24           [1, 128, 64, 64]             256\n",
      "             ReLU-25           [1, 128, 64, 64]               0\n",
      "           Conv2d-26           [1, 256, 64, 64]          33,024\n",
      "           Conv2d-27           [1, 256, 64, 64]          33,024\n",
      "       Bottleneck-28           [1, 256, 64, 64]               0\n",
      "      BatchNorm2d-29           [1, 256, 64, 64]             512\n",
      "             ReLU-30           [1, 256, 64, 64]               0\n",
      "           Conv2d-31           [1, 128, 64, 64]          32,896\n",
      "      BatchNorm2d-32           [1, 128, 64, 64]             256\n",
      "             ReLU-33           [1, 128, 64, 64]               0\n",
      "           Conv2d-34           [1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-35           [1, 128, 64, 64]             256\n",
      "             ReLU-36           [1, 128, 64, 64]               0\n",
      "           Conv2d-37           [1, 256, 64, 64]          33,024\n",
      "       Bottleneck-38           [1, 256, 64, 64]               0\n",
      "      BatchNorm2d-39           [1, 256, 64, 64]             512\n",
      "             ReLU-40           [1, 256, 64, 64]               0\n",
      "           Conv2d-41           [1, 128, 64, 64]          32,896\n",
      "      BatchNorm2d-42           [1, 128, 64, 64]             256\n",
      "             ReLU-43           [1, 128, 64, 64]               0\n",
      "           Conv2d-44           [1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-45           [1, 128, 64, 64]             256\n",
      "             ReLU-46           [1, 128, 64, 64]               0\n",
      "           Conv2d-47           [1, 256, 64, 64]          33,024\n",
      "       Bottleneck-48           [1, 256, 64, 64]               0\n",
      "      BatchNorm2d-49           [1, 256, 32, 32]             512\n",
      "             ReLU-50           [1, 256, 32, 32]               0\n",
      "           Conv2d-51           [1, 128, 32, 32]          32,896\n",
      "      BatchNorm2d-52           [1, 128, 32, 32]             256\n",
      "             ReLU-53           [1, 128, 32, 32]               0\n",
      "           Conv2d-54           [1, 128, 32, 32]         147,584\n",
      "      BatchNorm2d-55           [1, 128, 32, 32]             256\n",
      "             ReLU-56           [1, 128, 32, 32]               0\n",
      "           Conv2d-57           [1, 256, 32, 32]          33,024\n",
      "       Bottleneck-58           [1, 256, 32, 32]               0\n",
      "      BatchNorm2d-59           [1, 256, 32, 32]             512\n",
      "             ReLU-60           [1, 256, 32, 32]               0\n",
      "           Conv2d-61           [1, 128, 32, 32]          32,896\n",
      "      BatchNorm2d-62           [1, 128, 32, 32]             256\n",
      "             ReLU-63           [1, 128, 32, 32]               0\n",
      "           Conv2d-64           [1, 128, 32, 32]         147,584\n",
      "      BatchNorm2d-65           [1, 128, 32, 32]             256\n",
      "             ReLU-66           [1, 128, 32, 32]               0\n",
      "           Conv2d-67           [1, 256, 32, 32]          33,024\n",
      "       Bottleneck-68           [1, 256, 32, 32]               0\n",
      "      BatchNorm2d-69           [1, 256, 16, 16]             512\n",
      "             ReLU-70           [1, 256, 16, 16]               0\n",
      "           Conv2d-71           [1, 128, 16, 16]          32,896\n",
      "      BatchNorm2d-72           [1, 128, 16, 16]             256\n",
      "             ReLU-73           [1, 128, 16, 16]               0\n",
      "           Conv2d-74           [1, 128, 16, 16]         147,584\n",
      "      BatchNorm2d-75           [1, 128, 16, 16]             256\n",
      "             ReLU-76           [1, 128, 16, 16]               0\n",
      "           Conv2d-77           [1, 256, 16, 16]          33,024\n",
      "       Bottleneck-78           [1, 256, 16, 16]               0\n",
      "      BatchNorm2d-79           [1, 256, 16, 16]             512\n",
      "             ReLU-80           [1, 256, 16, 16]               0\n",
      "           Conv2d-81           [1, 128, 16, 16]          32,896\n",
      "      BatchNorm2d-82           [1, 128, 16, 16]             256\n",
      "             ReLU-83           [1, 128, 16, 16]               0\n",
      "           Conv2d-84           [1, 128, 16, 16]         147,584\n",
      "      BatchNorm2d-85           [1, 128, 16, 16]             256\n",
      "             ReLU-86           [1, 128, 16, 16]               0\n",
      "           Conv2d-87           [1, 256, 16, 16]          33,024\n",
      "       Bottleneck-88           [1, 256, 16, 16]               0\n",
      "      BatchNorm2d-89             [1, 256, 8, 8]             512\n",
      "             ReLU-90             [1, 256, 8, 8]               0\n",
      "           Conv2d-91             [1, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-92             [1, 128, 8, 8]             256\n",
      "             ReLU-93             [1, 128, 8, 8]               0\n",
      "           Conv2d-94             [1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-95             [1, 128, 8, 8]             256\n",
      "             ReLU-96             [1, 128, 8, 8]               0\n",
      "           Conv2d-97             [1, 256, 8, 8]          33,024\n",
      "       Bottleneck-98             [1, 256, 8, 8]               0\n",
      "      BatchNorm2d-99             [1, 256, 8, 8]             512\n",
      "            ReLU-100             [1, 256, 8, 8]               0\n",
      "          Conv2d-101             [1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-102             [1, 128, 8, 8]             256\n",
      "            ReLU-103             [1, 128, 8, 8]               0\n",
      "          Conv2d-104             [1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-105             [1, 128, 8, 8]             256\n",
      "            ReLU-106             [1, 128, 8, 8]               0\n",
      "          Conv2d-107             [1, 256, 8, 8]          33,024\n",
      "      Bottleneck-108             [1, 256, 8, 8]               0\n",
      "     BatchNorm2d-109             [1, 256, 4, 4]             512\n",
      "            ReLU-110             [1, 256, 4, 4]               0\n",
      "          Conv2d-111             [1, 128, 4, 4]          32,896\n",
      "     BatchNorm2d-112             [1, 128, 4, 4]             256\n",
      "            ReLU-113             [1, 128, 4, 4]               0\n",
      "          Conv2d-114             [1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-115             [1, 128, 4, 4]             256\n",
      "            ReLU-116             [1, 128, 4, 4]               0\n",
      "          Conv2d-117             [1, 256, 4, 4]          33,024\n",
      "      Bottleneck-118             [1, 256, 4, 4]               0\n",
      "     BatchNorm2d-119             [1, 256, 4, 4]             512\n",
      "            ReLU-120             [1, 256, 4, 4]               0\n",
      "          Conv2d-121             [1, 128, 4, 4]          32,896\n",
      "     BatchNorm2d-122             [1, 128, 4, 4]             256\n",
      "            ReLU-123             [1, 128, 4, 4]               0\n",
      "          Conv2d-124             [1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-125             [1, 128, 4, 4]             256\n",
      "            ReLU-126             [1, 128, 4, 4]               0\n",
      "          Conv2d-127             [1, 256, 4, 4]          33,024\n",
      "      Bottleneck-128             [1, 256, 4, 4]               0\n",
      "     BatchNorm2d-129             [1, 256, 4, 4]             512\n",
      "            ReLU-130             [1, 256, 4, 4]               0\n",
      "          Conv2d-131             [1, 128, 4, 4]          32,896\n",
      "     BatchNorm2d-132             [1, 128, 4, 4]             256\n",
      "            ReLU-133             [1, 128, 4, 4]               0\n",
      "          Conv2d-134             [1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-135             [1, 128, 4, 4]             256\n",
      "            ReLU-136             [1, 128, 4, 4]               0\n",
      "          Conv2d-137             [1, 256, 4, 4]          33,024\n",
      "      Bottleneck-138             [1, 256, 4, 4]               0\n",
      "     BatchNorm2d-139             [1, 256, 8, 8]             512\n",
      "            ReLU-140             [1, 256, 8, 8]               0\n",
      "          Conv2d-141             [1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-142             [1, 128, 8, 8]             256\n",
      "            ReLU-143             [1, 128, 8, 8]               0\n",
      "          Conv2d-144             [1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-145             [1, 128, 8, 8]             256\n",
      "            ReLU-146             [1, 128, 8, 8]               0\n",
      "          Conv2d-147             [1, 256, 8, 8]          33,024\n",
      "      Bottleneck-148             [1, 256, 8, 8]               0\n",
      "     BatchNorm2d-149           [1, 256, 16, 16]             512\n",
      "            ReLU-150           [1, 256, 16, 16]               0\n",
      "          Conv2d-151           [1, 128, 16, 16]          32,896\n",
      "     BatchNorm2d-152           [1, 128, 16, 16]             256\n",
      "            ReLU-153           [1, 128, 16, 16]               0\n",
      "          Conv2d-154           [1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-155           [1, 128, 16, 16]             256\n",
      "            ReLU-156           [1, 128, 16, 16]               0\n",
      "          Conv2d-157           [1, 256, 16, 16]          33,024\n",
      "      Bottleneck-158           [1, 256, 16, 16]               0\n",
      "     BatchNorm2d-159           [1, 256, 32, 32]             512\n",
      "            ReLU-160           [1, 256, 32, 32]               0\n",
      "          Conv2d-161           [1, 128, 32, 32]          32,896\n",
      "     BatchNorm2d-162           [1, 128, 32, 32]             256\n",
      "            ReLU-163           [1, 128, 32, 32]               0\n",
      "          Conv2d-164           [1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-165           [1, 128, 32, 32]             256\n",
      "            ReLU-166           [1, 128, 32, 32]               0\n",
      "          Conv2d-167           [1, 256, 32, 32]          33,024\n",
      "      Bottleneck-168           [1, 256, 32, 32]               0\n",
      "       Hourglass-169           [1, 256, 64, 64]               0\n",
      "     BatchNorm2d-170           [1, 256, 64, 64]             512\n",
      "            ReLU-171           [1, 256, 64, 64]               0\n",
      "          Conv2d-172           [1, 128, 64, 64]          32,896\n",
      "     BatchNorm2d-173           [1, 128, 64, 64]             256\n",
      "            ReLU-174           [1, 128, 64, 64]               0\n",
      "          Conv2d-175           [1, 128, 64, 64]         147,584\n",
      "     BatchNorm2d-176           [1, 128, 64, 64]             256\n",
      "            ReLU-177           [1, 128, 64, 64]               0\n",
      "          Conv2d-178           [1, 256, 64, 64]          33,024\n",
      "      Bottleneck-179           [1, 256, 64, 64]               0\n",
      "          Conv2d-180           [1, 256, 64, 64]          65,792\n",
      "     BatchNorm2d-181           [1, 256, 64, 64]             512\n",
      "            ReLU-182           [1, 256, 64, 64]               0\n",
      "            ReLU-183           [1, 256, 64, 64]               0\n",
      "            ReLU-184           [1, 256, 64, 64]               0\n",
      "          Conv2d-185            [1, 16, 64, 64]           4,112\n",
      "          Conv2d-186           [1, 256, 64, 64]          65,792\n",
      "          Conv2d-187           [1, 256, 64, 64]           4,352\n",
      "     BatchNorm2d-188           [1, 256, 64, 64]             512\n",
      "            ReLU-189           [1, 256, 64, 64]               0\n",
      "          Conv2d-190           [1, 128, 64, 64]          32,896\n",
      "     BatchNorm2d-191           [1, 128, 64, 64]             256\n",
      "            ReLU-192           [1, 128, 64, 64]               0\n",
      "          Conv2d-193           [1, 128, 64, 64]         147,584\n",
      "     BatchNorm2d-194           [1, 128, 64, 64]             256\n",
      "            ReLU-195           [1, 128, 64, 64]               0\n",
      "          Conv2d-196           [1, 256, 64, 64]          33,024\n",
      "      Bottleneck-197           [1, 256, 64, 64]               0\n",
      "     BatchNorm2d-198           [1, 256, 32, 32]             512\n",
      "            ReLU-199           [1, 256, 32, 32]               0\n",
      "          Conv2d-200           [1, 128, 32, 32]          32,896\n",
      "     BatchNorm2d-201           [1, 128, 32, 32]             256\n",
      "            ReLU-202           [1, 128, 32, 32]               0\n",
      "          Conv2d-203           [1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-204           [1, 128, 32, 32]             256\n",
      "            ReLU-205           [1, 128, 32, 32]               0\n",
      "          Conv2d-206           [1, 256, 32, 32]          33,024\n",
      "      Bottleneck-207           [1, 256, 32, 32]               0\n",
      "     BatchNorm2d-208           [1, 256, 32, 32]             512\n",
      "            ReLU-209           [1, 256, 32, 32]               0\n",
      "          Conv2d-210           [1, 128, 32, 32]          32,896\n",
      "     BatchNorm2d-211           [1, 128, 32, 32]             256\n",
      "            ReLU-212           [1, 128, 32, 32]               0\n",
      "          Conv2d-213           [1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-214           [1, 128, 32, 32]             256\n",
      "            ReLU-215           [1, 128, 32, 32]               0\n",
      "          Conv2d-216           [1, 256, 32, 32]          33,024\n",
      "      Bottleneck-217           [1, 256, 32, 32]               0\n",
      "     BatchNorm2d-218           [1, 256, 16, 16]             512\n",
      "            ReLU-219           [1, 256, 16, 16]               0\n",
      "          Conv2d-220           [1, 128, 16, 16]          32,896\n",
      "     BatchNorm2d-221           [1, 128, 16, 16]             256\n",
      "            ReLU-222           [1, 128, 16, 16]               0\n",
      "          Conv2d-223           [1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-224           [1, 128, 16, 16]             256\n",
      "            ReLU-225           [1, 128, 16, 16]               0\n",
      "          Conv2d-226           [1, 256, 16, 16]          33,024\n",
      "      Bottleneck-227           [1, 256, 16, 16]               0\n",
      "     BatchNorm2d-228           [1, 256, 16, 16]             512\n",
      "            ReLU-229           [1, 256, 16, 16]               0\n",
      "          Conv2d-230           [1, 128, 16, 16]          32,896\n",
      "     BatchNorm2d-231           [1, 128, 16, 16]             256\n",
      "            ReLU-232           [1, 128, 16, 16]               0\n",
      "          Conv2d-233           [1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-234           [1, 128, 16, 16]             256\n",
      "            ReLU-235           [1, 128, 16, 16]               0\n",
      "          Conv2d-236           [1, 256, 16, 16]          33,024\n",
      "      Bottleneck-237           [1, 256, 16, 16]               0\n",
      "     BatchNorm2d-238             [1, 256, 8, 8]             512\n",
      "            ReLU-239             [1, 256, 8, 8]               0\n",
      "          Conv2d-240             [1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-241             [1, 128, 8, 8]             256\n",
      "            ReLU-242             [1, 128, 8, 8]               0\n",
      "          Conv2d-243             [1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-244             [1, 128, 8, 8]             256\n",
      "            ReLU-245             [1, 128, 8, 8]               0\n",
      "          Conv2d-246             [1, 256, 8, 8]          33,024\n",
      "      Bottleneck-247             [1, 256, 8, 8]               0\n",
      "     BatchNorm2d-248             [1, 256, 8, 8]             512\n",
      "            ReLU-249             [1, 256, 8, 8]               0\n",
      "          Conv2d-250             [1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-251             [1, 128, 8, 8]             256\n",
      "            ReLU-252             [1, 128, 8, 8]               0\n",
      "          Conv2d-253             [1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-254             [1, 128, 8, 8]             256\n",
      "            ReLU-255             [1, 128, 8, 8]               0\n",
      "          Conv2d-256             [1, 256, 8, 8]          33,024\n",
      "      Bottleneck-257             [1, 256, 8, 8]               0\n",
      "     BatchNorm2d-258             [1, 256, 4, 4]             512\n",
      "            ReLU-259             [1, 256, 4, 4]               0\n",
      "          Conv2d-260             [1, 128, 4, 4]          32,896\n",
      "     BatchNorm2d-261             [1, 128, 4, 4]             256\n",
      "            ReLU-262             [1, 128, 4, 4]               0\n",
      "          Conv2d-263             [1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-264             [1, 128, 4, 4]             256\n",
      "            ReLU-265             [1, 128, 4, 4]               0\n",
      "          Conv2d-266             [1, 256, 4, 4]          33,024\n",
      "      Bottleneck-267             [1, 256, 4, 4]               0\n",
      "     BatchNorm2d-268             [1, 256, 4, 4]             512\n",
      "            ReLU-269             [1, 256, 4, 4]               0\n",
      "          Conv2d-270             [1, 128, 4, 4]          32,896\n",
      "     BatchNorm2d-271             [1, 128, 4, 4]             256\n",
      "            ReLU-272             [1, 128, 4, 4]               0\n",
      "          Conv2d-273             [1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-274             [1, 128, 4, 4]             256\n",
      "            ReLU-275             [1, 128, 4, 4]               0\n",
      "          Conv2d-276             [1, 256, 4, 4]          33,024\n",
      "      Bottleneck-277             [1, 256, 4, 4]               0\n",
      "     BatchNorm2d-278             [1, 256, 4, 4]             512\n",
      "            ReLU-279             [1, 256, 4, 4]               0\n",
      "          Conv2d-280             [1, 128, 4, 4]          32,896\n",
      "     BatchNorm2d-281             [1, 128, 4, 4]             256\n",
      "            ReLU-282             [1, 128, 4, 4]               0\n",
      "          Conv2d-283             [1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-284             [1, 128, 4, 4]             256\n",
      "            ReLU-285             [1, 128, 4, 4]               0\n",
      "          Conv2d-286             [1, 256, 4, 4]          33,024\n",
      "      Bottleneck-287             [1, 256, 4, 4]               0\n",
      "     BatchNorm2d-288             [1, 256, 8, 8]             512\n",
      "            ReLU-289             [1, 256, 8, 8]               0\n",
      "          Conv2d-290             [1, 128, 8, 8]          32,896\n",
      "     BatchNorm2d-291             [1, 128, 8, 8]             256\n",
      "            ReLU-292             [1, 128, 8, 8]               0\n",
      "          Conv2d-293             [1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-294             [1, 128, 8, 8]             256\n",
      "            ReLU-295             [1, 128, 8, 8]               0\n",
      "          Conv2d-296             [1, 256, 8, 8]          33,024\n",
      "      Bottleneck-297             [1, 256, 8, 8]               0\n",
      "     BatchNorm2d-298           [1, 256, 16, 16]             512\n",
      "            ReLU-299           [1, 256, 16, 16]               0\n",
      "          Conv2d-300           [1, 128, 16, 16]          32,896\n",
      "     BatchNorm2d-301           [1, 128, 16, 16]             256\n",
      "            ReLU-302           [1, 128, 16, 16]               0\n",
      "          Conv2d-303           [1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-304           [1, 128, 16, 16]             256\n",
      "            ReLU-305           [1, 128, 16, 16]               0\n",
      "          Conv2d-306           [1, 256, 16, 16]          33,024\n",
      "      Bottleneck-307           [1, 256, 16, 16]               0\n",
      "     BatchNorm2d-308           [1, 256, 32, 32]             512\n",
      "            ReLU-309           [1, 256, 32, 32]               0\n",
      "          Conv2d-310           [1, 128, 32, 32]          32,896\n",
      "     BatchNorm2d-311           [1, 128, 32, 32]             256\n",
      "            ReLU-312           [1, 128, 32, 32]               0\n",
      "          Conv2d-313           [1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-314           [1, 128, 32, 32]             256\n",
      "            ReLU-315           [1, 128, 32, 32]               0\n",
      "          Conv2d-316           [1, 256, 32, 32]          33,024\n",
      "      Bottleneck-317           [1, 256, 32, 32]               0\n",
      "       Hourglass-318           [1, 256, 64, 64]               0\n",
      "     BatchNorm2d-319           [1, 256, 64, 64]             512\n",
      "            ReLU-320           [1, 256, 64, 64]               0\n",
      "          Conv2d-321           [1, 128, 64, 64]          32,896\n",
      "     BatchNorm2d-322           [1, 128, 64, 64]             256\n",
      "            ReLU-323           [1, 128, 64, 64]               0\n",
      "          Conv2d-324           [1, 128, 64, 64]         147,584\n",
      "     BatchNorm2d-325           [1, 128, 64, 64]             256\n",
      "            ReLU-326           [1, 128, 64, 64]               0\n",
      "          Conv2d-327           [1, 256, 64, 64]          33,024\n",
      "      Bottleneck-328           [1, 256, 64, 64]               0\n",
      "          Conv2d-329           [1, 256, 64, 64]          65,792\n",
      "     BatchNorm2d-330           [1, 256, 64, 64]             512\n",
      "            ReLU-331           [1, 256, 64, 64]               0\n",
      "            ReLU-332           [1, 256, 64, 64]               0\n",
      "            ReLU-333           [1, 256, 64, 64]               0\n",
      "          Conv2d-334            [1, 16, 64, 64]           4,112\n",
      "================================================================\n",
      "Total params: 6,730,912\n",
      "Trainable params: 6,730,912\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 716.56\n",
      "Params size (MB): 25.68\n",
      "Estimated Total Size (MB): 742.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2 stacked hourglass\n",
    "model = HourglassNet(Bottleneck, num_stacks=2, num_blocks=1, num_classes=16).to(device)\n",
    "\n",
    "summary(model, input_size=(3, 256, 256), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Baselines for Human Pose Estimation and Tracking (2018)\n",
    "\n",
    "#### Single-Stage Architecture\n",
    "\n",
    "<img src=\"./src_imgs/13.png\" width=800>\n",
    "\n",
    "한편, \"Simple Baselines for Human Pose Estimation and Tracking\" 논문의 저자들은 기존 Multi-Stage 구조의 모델들의 알고리즘적인 복잡성에 주목했습니다. 기존 Multi-Stage 의 비효율적이고 파악하기 어려운 모델 구조는 네트워크를 설계하는 것에 있어 복잡도를 크게 증가시켰습니다. 이러한 점에서 저자는 Backbone Network에 Deconvolution layer와 간단한 Upsampling을 추가하는 것만으로도 기존 모델들에 버금가는 성능을 가질 수 있음을 제시하였습니다. \n",
    "\n",
    "위 그림에서 (a)는 Hourglass 네트워크를 나타내는데, 저자가 제시한 (c) 모델에 비해 상당히 간단한 구조를 갖습니다.\n",
    "\n",
    "#### Backbone Network\n",
    "\n",
    "<img src=\"./src_imgs/14.png\" height=400>\n",
    "\n",
    "저자의 목표는 \"간단한 방법이 얼마나 좋은 성능을 낼 수 있는가?\" 에 대한 답을 얻는 것이었습니다. 저자는 대규모 이미지 데이터셋인 imagenet에서 classification task를 위해 훈련된 ResNet을 Backbone으로 사용하였습니다. 앞서 Hourglass에서 사용한 Residual Block은 사실 ResNet에서 처음 제시되었습니다. 저자는 ResNet을 이용해 이미지의 특징을 추출하면 이러한 features로부터 포즈 역시 잘 파악할 수 있다는 전제하에 사용하였습니다. 실제로 대규모 데이터셋에서 학습된 네트워크를 feature extractor (or backbone)로써 사용하는 경우가 많으며, 비슷한 task에서 좋은 성능을 보입니다. \n",
    "\n",
    "imagenet에서 학습된 ResNet 네트워크는 가장 마지막 layer로 classification을 수행하는 fully connected layer (fc layer)를 가집니다. 논문에서는 feature extractor로써만 사용하기 위해 fc layer는 떼어내어 버리고 제안된 layer만 붙이는 구조를 제안합니다.\n",
    "\n",
    "#### Deconvolution layer\n",
    "\n",
    "|![](https://cdn-images-1.medium.com/max/1200/1*BMngs93_rm2_BpJFH2mS0Q.gif)|\n",
    "|:---:|\n",
    "|*2D convolution with no padding, stride of 2 and kernel of 3*|\n",
    "\n",
    "|![](https://cdn-images-1.medium.com/max/1200/1*Lpn4nag_KRMfGkx1k6bV-g.gif)|\n",
    "|:---:|\n",
    "|*Transposed 2D convolution with no padding, stride of 2 and kernel of 3*|\n",
    "\n",
    "\n",
    "논문에선 Deconvolution 이라고 언급하고 있지만 실제 연산은 Deconvolution이 아닌 Transposed Convolution을 수행합니다. Deconvolution은 사실 Upsampling을 수행하기 위해 Convolution의 역연산 과정을 취한 것입니다. 반면, Transposed Convolution은 Deconvolution과 동일한 Resolution을 복구할 뿐 수학적인 연산 자체는 다릅니다. 이미지의 다양한 scale에서 정보를 추출하기 위한 Upsampling 방법 중 하나로 Convolution과 같이 학습 가능한 parameters가 존재합니다. \n",
    "\n",
    "앞서 Hourglass에선 Upsampling 과정으로 Nearest Neighbour Interpolation을 사용한 반면, 저자는 Deconvolution (Transposed Convolution)으로 수행했다는 점 정도로 기억하면 좋을것 같습니다. \n",
    "\n",
    "더 자세한 내용은 Reference 에 관련 링크를 참고하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 정의 및 구조 시각화\n",
    "\n",
    "이제 ResNet-101 Backbone을 사용하여 Simple Baseline 모델을 정의하겠습니다.\n",
    "\n",
    "먼저, ResNet-101에서 사용하는 Residual Block을 정의합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "BN_MOMENTUM = 0.1\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n",
    "                                  momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-101과 Upsampling process (Deconvolution)를 통합한 전체 네트워크를 정의합니다. \n",
    "`forward()`를 보면 ResNet을 거쳐 마지막에 Deconvolution layer와 final layer만 추가한 것을 확인할 수 있습니다.\n",
    "final layer는 1x1 convolution입니다. output channel을 joints 개수로 맞추어 heatmap을 생성함을 알 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_spec = {101: (Bottleneck, [3, 4, 23, 3])}\n",
    "\n",
    "\n",
    "def get_pose_net(cfg, is_train, **kwargs):\n",
    "    num_layers = cfg.MODEL.EXTRA.NUM_LAYERS\n",
    "\n",
    "    block_class, layers = resnet_spec[num_layers]\n",
    "\n",
    "    model = PoseResNet(block_class, layers, cfg, **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class PoseResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, cfg, **kwargs):\n",
    "        self.inplanes = 64\n",
    "        extra = cfg.MODEL.EXTRA\n",
    "        self.deconv_with_bias = extra.DECONV_WITH_BIAS\n",
    "\n",
    "        super(PoseResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # used for deconv layers\n",
    "        self.deconv_layers = self._make_deconv_layer(\n",
    "            extra.NUM_DECONV_LAYERS,\n",
    "            extra.NUM_DECONV_FILTERS,\n",
    "            extra.NUM_DECONV_KERNELS,\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Conv2d(\n",
    "            in_channels=extra.NUM_DECONV_FILTERS[-1],\n",
    "            out_channels=cfg.MODEL.NUM_JOINTS,\n",
    "            kernel_size=extra.FINAL_CONV_KERNEL,\n",
    "            stride=1,\n",
    "            padding=1 if extra.FINAL_CONV_KERNEL == 3 else 0\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _get_deconv_cfg(self, deconv_kernel, index):\n",
    "        if deconv_kernel == 4:\n",
    "            padding = 1\n",
    "            output_padding = 0\n",
    "        elif deconv_kernel == 3:\n",
    "            padding = 1\n",
    "            output_padding = 1\n",
    "        elif deconv_kernel == 2:\n",
    "            padding = 0\n",
    "            output_padding = 0\n",
    "\n",
    "        return deconv_kernel, padding, output_padding\n",
    "\n",
    "    def _make_deconv_layer(self, num_layers, num_filters, num_kernels):\n",
    "        assert num_layers == len(num_filters), \\\n",
    "            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n",
    "        assert num_layers == len(num_kernels), \\\n",
    "            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            kernel, padding, output_padding = \\\n",
    "                self._get_deconv_cfg(num_kernels[i], i)\n",
    "\n",
    "            planes = num_filters[i]\n",
    "            layers.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=self.inplanes,\n",
    "                    out_channels=planes,\n",
    "                    kernel_size=kernel,\n",
    "                    stride=2,\n",
    "                    padding=padding,\n",
    "                    output_padding=output_padding,\n",
    "                    bias=self.deconv_with_bias))\n",
    "            layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            self.inplanes = planes\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.deconv_layers(x)\n",
    "        x = self.final_layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 ResNet-101을 Backbone으로 사용하는 PoseResNet의 구조를 출력해보겠습니다.\n",
    "\n",
    "PoseResNet이 사용하는 Backbone인 ResNet 역시 가벼운 네트워크는 아닙니다.\n",
    "제안된 방법은 단순히 파라미터를 줄이려는 것이 아니라, 기존의 down, up sampling을 반복적으로 해야한다는 기틀에서 벗어나 마지막 단에만 Upsampling 과정을 추가하는 것으로 좋은 성능을 낼 수 있다는 점을 보였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-2          [1, 64, 128, 128]             128\n",
      "              ReLU-3          [1, 64, 128, 128]               0\n",
      "         MaxPool2d-4            [1, 64, 64, 64]               0\n",
      "            Conv2d-5            [1, 64, 64, 64]           4,096\n",
      "       BatchNorm2d-6            [1, 64, 64, 64]             128\n",
      "              ReLU-7            [1, 64, 64, 64]               0\n",
      "            Conv2d-8            [1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9            [1, 64, 64, 64]             128\n",
      "             ReLU-10            [1, 64, 64, 64]               0\n",
      "           Conv2d-11           [1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-12           [1, 256, 64, 64]             512\n",
      "           Conv2d-13           [1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-14           [1, 256, 64, 64]             512\n",
      "             ReLU-15           [1, 256, 64, 64]               0\n",
      "       Bottleneck-16           [1, 256, 64, 64]               0\n",
      "           Conv2d-17            [1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-18            [1, 64, 64, 64]             128\n",
      "             ReLU-19            [1, 64, 64, 64]               0\n",
      "           Conv2d-20            [1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-21            [1, 64, 64, 64]             128\n",
      "             ReLU-22            [1, 64, 64, 64]               0\n",
      "           Conv2d-23           [1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-24           [1, 256, 64, 64]             512\n",
      "             ReLU-25           [1, 256, 64, 64]               0\n",
      "       Bottleneck-26           [1, 256, 64, 64]               0\n",
      "           Conv2d-27            [1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-28            [1, 64, 64, 64]             128\n",
      "             ReLU-29            [1, 64, 64, 64]               0\n",
      "           Conv2d-30            [1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-31            [1, 64, 64, 64]             128\n",
      "             ReLU-32            [1, 64, 64, 64]               0\n",
      "           Conv2d-33           [1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-34           [1, 256, 64, 64]             512\n",
      "             ReLU-35           [1, 256, 64, 64]               0\n",
      "       Bottleneck-36           [1, 256, 64, 64]               0\n",
      "           Conv2d-37           [1, 128, 64, 64]          32,768\n",
      "      BatchNorm2d-38           [1, 128, 64, 64]             256\n",
      "             ReLU-39           [1, 128, 64, 64]               0\n",
      "           Conv2d-40           [1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-41           [1, 128, 32, 32]             256\n",
      "             ReLU-42           [1, 128, 32, 32]               0\n",
      "           Conv2d-43           [1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-44           [1, 512, 32, 32]           1,024\n",
      "           Conv2d-45           [1, 512, 32, 32]         131,072\n",
      "      BatchNorm2d-46           [1, 512, 32, 32]           1,024\n",
      "             ReLU-47           [1, 512, 32, 32]               0\n",
      "       Bottleneck-48           [1, 512, 32, 32]               0\n",
      "           Conv2d-49           [1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-50           [1, 128, 32, 32]             256\n",
      "             ReLU-51           [1, 128, 32, 32]               0\n",
      "           Conv2d-52           [1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-53           [1, 128, 32, 32]             256\n",
      "             ReLU-54           [1, 128, 32, 32]               0\n",
      "           Conv2d-55           [1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-56           [1, 512, 32, 32]           1,024\n",
      "             ReLU-57           [1, 512, 32, 32]               0\n",
      "       Bottleneck-58           [1, 512, 32, 32]               0\n",
      "           Conv2d-59           [1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-60           [1, 128, 32, 32]             256\n",
      "             ReLU-61           [1, 128, 32, 32]               0\n",
      "           Conv2d-62           [1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-63           [1, 128, 32, 32]             256\n",
      "             ReLU-64           [1, 128, 32, 32]               0\n",
      "           Conv2d-65           [1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-66           [1, 512, 32, 32]           1,024\n",
      "             ReLU-67           [1, 512, 32, 32]               0\n",
      "       Bottleneck-68           [1, 512, 32, 32]               0\n",
      "           Conv2d-69           [1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-70           [1, 128, 32, 32]             256\n",
      "             ReLU-71           [1, 128, 32, 32]               0\n",
      "           Conv2d-72           [1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-73           [1, 128, 32, 32]             256\n",
      "             ReLU-74           [1, 128, 32, 32]               0\n",
      "           Conv2d-75           [1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-76           [1, 512, 32, 32]           1,024\n",
      "             ReLU-77           [1, 512, 32, 32]               0\n",
      "       Bottleneck-78           [1, 512, 32, 32]               0\n",
      "           Conv2d-79           [1, 256, 32, 32]         131,072\n",
      "      BatchNorm2d-80           [1, 256, 32, 32]             512\n",
      "             ReLU-81           [1, 256, 32, 32]               0\n",
      "           Conv2d-82           [1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-83           [1, 256, 16, 16]             512\n",
      "             ReLU-84           [1, 256, 16, 16]               0\n",
      "           Conv2d-85          [1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-86          [1, 1024, 16, 16]           2,048\n",
      "           Conv2d-87          [1, 1024, 16, 16]         524,288\n",
      "      BatchNorm2d-88          [1, 1024, 16, 16]           2,048\n",
      "             ReLU-89          [1, 1024, 16, 16]               0\n",
      "       Bottleneck-90          [1, 1024, 16, 16]               0\n",
      "           Conv2d-91           [1, 256, 16, 16]         262,144\n",
      "      BatchNorm2d-92           [1, 256, 16, 16]             512\n",
      "             ReLU-93           [1, 256, 16, 16]               0\n",
      "           Conv2d-94           [1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-95           [1, 256, 16, 16]             512\n",
      "             ReLU-96           [1, 256, 16, 16]               0\n",
      "           Conv2d-97          [1, 1024, 16, 16]         262,144\n",
      "      BatchNorm2d-98          [1, 1024, 16, 16]           2,048\n",
      "             ReLU-99          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-100          [1, 1024, 16, 16]               0\n",
      "          Conv2d-101           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-102           [1, 256, 16, 16]             512\n",
      "            ReLU-103           [1, 256, 16, 16]               0\n",
      "          Conv2d-104           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-105           [1, 256, 16, 16]             512\n",
      "            ReLU-106           [1, 256, 16, 16]               0\n",
      "          Conv2d-107          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-108          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-109          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-110          [1, 1024, 16, 16]               0\n",
      "          Conv2d-111           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-112           [1, 256, 16, 16]             512\n",
      "            ReLU-113           [1, 256, 16, 16]               0\n",
      "          Conv2d-114           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-115           [1, 256, 16, 16]             512\n",
      "            ReLU-116           [1, 256, 16, 16]               0\n",
      "          Conv2d-117          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-118          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-119          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-120          [1, 1024, 16, 16]               0\n",
      "          Conv2d-121           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-122           [1, 256, 16, 16]             512\n",
      "            ReLU-123           [1, 256, 16, 16]               0\n",
      "          Conv2d-124           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-125           [1, 256, 16, 16]             512\n",
      "            ReLU-126           [1, 256, 16, 16]               0\n",
      "          Conv2d-127          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-128          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-129          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-130          [1, 1024, 16, 16]               0\n",
      "          Conv2d-131           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-132           [1, 256, 16, 16]             512\n",
      "            ReLU-133           [1, 256, 16, 16]               0\n",
      "          Conv2d-134           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-135           [1, 256, 16, 16]             512\n",
      "            ReLU-136           [1, 256, 16, 16]               0\n",
      "          Conv2d-137          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-138          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-139          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-140          [1, 1024, 16, 16]               0\n",
      "          Conv2d-141           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-142           [1, 256, 16, 16]             512\n",
      "            ReLU-143           [1, 256, 16, 16]               0\n",
      "          Conv2d-144           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-145           [1, 256, 16, 16]             512\n",
      "            ReLU-146           [1, 256, 16, 16]               0\n",
      "          Conv2d-147          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-148          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-149          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-150          [1, 1024, 16, 16]               0\n",
      "          Conv2d-151           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-152           [1, 256, 16, 16]             512\n",
      "            ReLU-153           [1, 256, 16, 16]               0\n",
      "          Conv2d-154           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-155           [1, 256, 16, 16]             512\n",
      "            ReLU-156           [1, 256, 16, 16]               0\n",
      "          Conv2d-157          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-158          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-159          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-160          [1, 1024, 16, 16]               0\n",
      "          Conv2d-161           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-162           [1, 256, 16, 16]             512\n",
      "            ReLU-163           [1, 256, 16, 16]               0\n",
      "          Conv2d-164           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-165           [1, 256, 16, 16]             512\n",
      "            ReLU-166           [1, 256, 16, 16]               0\n",
      "          Conv2d-167          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-168          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-169          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-170          [1, 1024, 16, 16]               0\n",
      "          Conv2d-171           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-172           [1, 256, 16, 16]             512\n",
      "            ReLU-173           [1, 256, 16, 16]               0\n",
      "          Conv2d-174           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-175           [1, 256, 16, 16]             512\n",
      "            ReLU-176           [1, 256, 16, 16]               0\n",
      "          Conv2d-177          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-178          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-179          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-180          [1, 1024, 16, 16]               0\n",
      "          Conv2d-181           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-182           [1, 256, 16, 16]             512\n",
      "            ReLU-183           [1, 256, 16, 16]               0\n",
      "          Conv2d-184           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-185           [1, 256, 16, 16]             512\n",
      "            ReLU-186           [1, 256, 16, 16]               0\n",
      "          Conv2d-187          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-188          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-189          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-190          [1, 1024, 16, 16]               0\n",
      "          Conv2d-191           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-192           [1, 256, 16, 16]             512\n",
      "            ReLU-193           [1, 256, 16, 16]               0\n",
      "          Conv2d-194           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-195           [1, 256, 16, 16]             512\n",
      "            ReLU-196           [1, 256, 16, 16]               0\n",
      "          Conv2d-197          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-198          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-199          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-200          [1, 1024, 16, 16]               0\n",
      "          Conv2d-201           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-202           [1, 256, 16, 16]             512\n",
      "            ReLU-203           [1, 256, 16, 16]               0\n",
      "          Conv2d-204           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-205           [1, 256, 16, 16]             512\n",
      "            ReLU-206           [1, 256, 16, 16]               0\n",
      "          Conv2d-207          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-208          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-209          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-210          [1, 1024, 16, 16]               0\n",
      "          Conv2d-211           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-212           [1, 256, 16, 16]             512\n",
      "            ReLU-213           [1, 256, 16, 16]               0\n",
      "          Conv2d-214           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-215           [1, 256, 16, 16]             512\n",
      "            ReLU-216           [1, 256, 16, 16]               0\n",
      "          Conv2d-217          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-218          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-219          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-220          [1, 1024, 16, 16]               0\n",
      "          Conv2d-221           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-222           [1, 256, 16, 16]             512\n",
      "            ReLU-223           [1, 256, 16, 16]               0\n",
      "          Conv2d-224           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-225           [1, 256, 16, 16]             512\n",
      "            ReLU-226           [1, 256, 16, 16]               0\n",
      "          Conv2d-227          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-228          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-229          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-230          [1, 1024, 16, 16]               0\n",
      "          Conv2d-231           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-232           [1, 256, 16, 16]             512\n",
      "            ReLU-233           [1, 256, 16, 16]               0\n",
      "          Conv2d-234           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-235           [1, 256, 16, 16]             512\n",
      "            ReLU-236           [1, 256, 16, 16]               0\n",
      "          Conv2d-237          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-238          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-239          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-240          [1, 1024, 16, 16]               0\n",
      "          Conv2d-241           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-242           [1, 256, 16, 16]             512\n",
      "            ReLU-243           [1, 256, 16, 16]               0\n",
      "          Conv2d-244           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-245           [1, 256, 16, 16]             512\n",
      "            ReLU-246           [1, 256, 16, 16]               0\n",
      "          Conv2d-247          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-248          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-249          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-250          [1, 1024, 16, 16]               0\n",
      "          Conv2d-251           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-252           [1, 256, 16, 16]             512\n",
      "            ReLU-253           [1, 256, 16, 16]               0\n",
      "          Conv2d-254           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-255           [1, 256, 16, 16]             512\n",
      "            ReLU-256           [1, 256, 16, 16]               0\n",
      "          Conv2d-257          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-258          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-259          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-260          [1, 1024, 16, 16]               0\n",
      "          Conv2d-261           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-262           [1, 256, 16, 16]             512\n",
      "            ReLU-263           [1, 256, 16, 16]               0\n",
      "          Conv2d-264           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-265           [1, 256, 16, 16]             512\n",
      "            ReLU-266           [1, 256, 16, 16]               0\n",
      "          Conv2d-267          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-268          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-269          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-270          [1, 1024, 16, 16]               0\n",
      "          Conv2d-271           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-272           [1, 256, 16, 16]             512\n",
      "            ReLU-273           [1, 256, 16, 16]               0\n",
      "          Conv2d-274           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-275           [1, 256, 16, 16]             512\n",
      "            ReLU-276           [1, 256, 16, 16]               0\n",
      "          Conv2d-277          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-278          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-279          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-280          [1, 1024, 16, 16]               0\n",
      "          Conv2d-281           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-282           [1, 256, 16, 16]             512\n",
      "            ReLU-283           [1, 256, 16, 16]               0\n",
      "          Conv2d-284           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-285           [1, 256, 16, 16]             512\n",
      "            ReLU-286           [1, 256, 16, 16]               0\n",
      "          Conv2d-287          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-288          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-289          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-290          [1, 1024, 16, 16]               0\n",
      "          Conv2d-291           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-292           [1, 256, 16, 16]             512\n",
      "            ReLU-293           [1, 256, 16, 16]               0\n",
      "          Conv2d-294           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-295           [1, 256, 16, 16]             512\n",
      "            ReLU-296           [1, 256, 16, 16]               0\n",
      "          Conv2d-297          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-298          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-299          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-300          [1, 1024, 16, 16]               0\n",
      "          Conv2d-301           [1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-302           [1, 256, 16, 16]             512\n",
      "            ReLU-303           [1, 256, 16, 16]               0\n",
      "          Conv2d-304           [1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-305           [1, 256, 16, 16]             512\n",
      "            ReLU-306           [1, 256, 16, 16]               0\n",
      "          Conv2d-307          [1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-308          [1, 1024, 16, 16]           2,048\n",
      "            ReLU-309          [1, 1024, 16, 16]               0\n",
      "      Bottleneck-310          [1, 1024, 16, 16]               0\n",
      "          Conv2d-311           [1, 512, 16, 16]         524,288\n",
      "     BatchNorm2d-312           [1, 512, 16, 16]           1,024\n",
      "            ReLU-313           [1, 512, 16, 16]               0\n",
      "          Conv2d-314             [1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-315             [1, 512, 8, 8]           1,024\n",
      "            ReLU-316             [1, 512, 8, 8]               0\n",
      "          Conv2d-317            [1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-318            [1, 2048, 8, 8]           4,096\n",
      "          Conv2d-319            [1, 2048, 8, 8]       2,097,152\n",
      "     BatchNorm2d-320            [1, 2048, 8, 8]           4,096\n",
      "            ReLU-321            [1, 2048, 8, 8]               0\n",
      "      Bottleneck-322            [1, 2048, 8, 8]               0\n",
      "          Conv2d-323             [1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-324             [1, 512, 8, 8]           1,024\n",
      "            ReLU-325             [1, 512, 8, 8]               0\n",
      "          Conv2d-326             [1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-327             [1, 512, 8, 8]           1,024\n",
      "            ReLU-328             [1, 512, 8, 8]               0\n",
      "          Conv2d-329            [1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-330            [1, 2048, 8, 8]           4,096\n",
      "            ReLU-331            [1, 2048, 8, 8]               0\n",
      "      Bottleneck-332            [1, 2048, 8, 8]               0\n",
      "          Conv2d-333             [1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-334             [1, 512, 8, 8]           1,024\n",
      "            ReLU-335             [1, 512, 8, 8]               0\n",
      "          Conv2d-336             [1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-337             [1, 512, 8, 8]           1,024\n",
      "            ReLU-338             [1, 512, 8, 8]               0\n",
      "          Conv2d-339            [1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-340            [1, 2048, 8, 8]           4,096\n",
      "            ReLU-341            [1, 2048, 8, 8]               0\n",
      "      Bottleneck-342            [1, 2048, 8, 8]               0\n",
      " ConvTranspose2d-343           [1, 256, 16, 16]       8,388,608\n",
      "     BatchNorm2d-344           [1, 256, 16, 16]             512\n",
      "            ReLU-345           [1, 256, 16, 16]               0\n",
      " ConvTranspose2d-346           [1, 256, 32, 32]       1,048,576\n",
      "     BatchNorm2d-347           [1, 256, 32, 32]             512\n",
      "            ReLU-348           [1, 256, 32, 32]               0\n",
      " ConvTranspose2d-349           [1, 256, 64, 64]       1,048,576\n",
      "     BatchNorm2d-350           [1, 256, 64, 64]             512\n",
      "            ReLU-351           [1, 256, 64, 64]               0\n",
      "          Conv2d-352            [1, 16, 64, 64]           4,112\n",
      "================================================================\n",
      "Total params: 52,991,568\n",
      "Trainable params: 52,991,568\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 593.25\n",
      "Params size (MB): 202.15\n",
      "Estimated Total Size (MB): 796.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cfg = edict()\n",
    "\n",
    "cfg.MODEL = edict()\n",
    "cfg.MODEL.NUM_JOINTS = 16\n",
    "\n",
    "cfg.MODEL.EXTRA = edict()\n",
    "cfg.MODEL.EXTRA.NUM_LAYERS = 101\n",
    "cfg.MODEL.EXTRA.NUM_DECONV_KERNELS = [4, 4, 4]\n",
    "cfg.MODEL.EXTRA.NUM_DECONV_FILTERS = [256, 256, 256]\n",
    "cfg.MODEL.EXTRA.NUM_DECONV_LAYERS = 3\n",
    "cfg.MODEL.EXTRA.DECONV_WITH_BIAS = False\n",
    "cfg.MODEL.EXTRA.FINAL_CONV_KERNEL = 1\n",
    "\n",
    "model = get_pose_net(cfg=cfg, is_train=False)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model=model, input_size=(3, 256, 256), batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt15",
   "language": "python",
   "name": "pt15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}