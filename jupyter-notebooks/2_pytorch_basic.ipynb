{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Document: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "참고 자료: [https://github.com/yunjey/pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial)\n",
    "\n",
    "## Pytorch Basic\n",
    "\n",
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1 tensor([[-1.1228e-31,  4.7084e-43, -1.1227e-31],\n        [ 4.7084e-43, -1.1228e-31,  4.7084e-43],\n        [-1.1228e-31,  4.7084e-43, -1.1227e-31],\n        [ 4.7084e-43, -1.1227e-31,  4.7084e-43],\n        [-1.1226e-31,  4.7084e-43, -1.1226e-31]])\n2 tensor([[0.8320, 0.6646, 0.9919],\n        [0.3743, 0.6154, 0.3584],\n        [0.4355, 0.6022, 0.9167],\n        [0.3451, 0.0590, 0.2357],\n        [0.8384, 0.7062, 0.3904]])\n3 tensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n4 tensor([5.5000, 3.0000])\n5 tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\n6 tensor([[ 0.4049, -1.1705,  1.5475],\n        [-0.2964,  1.4206, -0.0710],\n        [ 1.2949, -0.3629,  0.1995],\n        [-1.0843, -0.9313, -0.2033],\n        [ 1.0777, -0.3450, -0.6429]])\n7 torch.Size([5, 3])\n8 tensor([[ 1.2105, -0.7215,  2.4600],\n        [-0.1789,  1.4323, -0.0599],\n        [ 1.9490,  0.1294,  1.0124],\n        [-0.6365, -0.2488,  0.3141],\n        [ 1.7716,  0.2907, -0.4926]])\n9 tensor([[ 1.2105, -0.7215,  2.4600],\n        [-0.1789,  1.4323, -0.0599],\n        [ 1.9490,  0.1294,  1.0124],\n        [-0.6365, -0.2488,  0.3141],\n        [ 1.7716,  0.2907, -0.4926]])\n10 tensor([[ 1.2105, -0.7215,  2.4600],\n        [-0.1789,  1.4323, -0.0599],\n        [ 1.9490,  0.1294,  1.0124],\n        [-0.6365, -0.2488,  0.3141],\n        [ 1.7716,  0.2907, -0.4926]])\n11 tensor([-1.1705,  1.4206, -0.3629, -0.9313, -0.3450])\n12 torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n13 tensor([[ 0.7791, -1.7852,  1.1134,  0.3308],\n        [-0.7007,  0.4192, -0.8629, -0.2875],\n        [ 0.2497, -0.7679, -1.0111, -1.1089],\n        [-0.2698,  0.1279, -1.6158, -0.7511]]) tensor([ 0.7791, -1.7852,  1.1134,  0.3308, -0.7007,  0.4192, -0.8629, -0.2875,\n         0.2497, -0.7679, -1.0111, -1.1089, -0.2698,  0.1279, -1.6158, -0.7511]) tensor([[ 0.7791, -1.7852,  1.1134,  0.3308, -0.7007,  0.4192, -0.8629, -0.2875],\n        [ 0.2497, -0.7679, -1.0111, -1.1089, -0.2698,  0.1279, -1.6158, -0.7511]])\n14 tensor([0.7450])\n15 0.74495929479599\n16 tensor([1., 1., 1., 1., 1.])\n17 [1. 1. 1. 1. 1.]\n18 tensor([2., 2., 2., 2., 2.])\n19 [2. 2. 2. 2. 2.]\n20 [2. 2. 2. 2. 2.]\n21 tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n22 tensor([1.7450], device='cuda:0')\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tensors\n",
    "x = torch.empty(5, 3)\n",
    "print('1', x)\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print('2', x)\n",
    "\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print('3', x)\n",
    "\n",
    "x = torch.tensor([5.5, 3])\n",
    "print('4', x)\n",
    "\n",
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print('5', x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print('6', x)                                      # result has the same size\n",
    "\n",
    "print('7', x.size())\n",
    "\n",
    "# Operations\n",
    "y = torch.rand(5, 3)\n",
    "print('8', x + y)\n",
    "print('9', torch.add(x, y))\n",
    "\n",
    "y.add_(x) # Any operation that mutates a tensor in-place is post-fixed with an '_'. For example: x.copy_(y), x.t_(), will change x.\n",
    "print('10', y)\n",
    "\n",
    "print('11', x[:, 1])\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16) # reshape/resize\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print('12', x.size(), y.size(), z.size())\n",
    "print('13', x, y, z)\n",
    "\n",
    "x = torch.randn(1)\n",
    "print('14', x)\n",
    "print('15', x.item()) # If you have a one element tensor, use .item() to get the value as a Python number\n",
    "\n",
    "# Converting a Torch Tensor to a NumPy Array\n",
    "a = torch.ones(5)\n",
    "print('16', a)\n",
    "b = a.numpy()\n",
    "print('17', b)\n",
    "\n",
    "a.add_(1) # 같은 메모리를 공유한다.\n",
    "print('18', a)\n",
    "print('19', b)\n",
    "\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print('20', a)\n",
    "print('21', b)\n",
    "\n",
    "# CUDA Tensors\n",
    "# Tensors can be moved onto any device using the .to method.\n",
    "\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print('22', z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd 자동미분 기능\n",
    "\n",
    "Pytorch는 Tensor의 모든 연산에 대해 자동으로 미분을 제공합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(2.)\ntensor(1.)\ntensor(1.)\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensors.\n",
    "# requires_grad = True이면 backward()가 호출되었을 때 오차 역전파 과정을 위한 미분 계산을 허용합니다.\n",
    "x = torch.tensor(1., requires_grad=True) # requires_grad default: True\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 (w)\n",
    "print(w.grad)    # w.grad = 1 (x)\n",
    "print(b.grad)    # b.grad = 1 (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "다음 예시는 Linear regression 모델의 학습 과정을 간략히 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "w:  torch.Size([2, 3])\nb:  torch.Size([2])\nloss:  1.1797583103179932\ndL/dw:  tensor([[ 0.3637, -0.1612,  0.2321],\n        [ 0.5997,  0.0321,  0.3729]])\ndL/db:  tensor([-0.7479, -0.5895])\nloss after 1 step optimization:  1.1636903285980225\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3) # get random values from standard normal dirstribution\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight.shape) # (2, 3)인 이유는 효율적으로 행렬 연산하기위해 내부적으로 transpose 시키기 때문. 신경쓰지 않아도 됩니다.\n",
    "print ('b: ', linear.bias.shape) # (2)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss() # Mean Squared Error\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01) # parameters()에는 layer의 모든 weights, bais가 저장되어있습니다. \n",
    "# optimizer는 한번의 step()으로 모든 가중치들을 -gradient * learning rate 만큼 수정합니다. 즉, 오차역전파를 수행하는 최적화 기법(RMSProp, Momentum, etc)을 정의합니다.\n",
    "\n",
    "# Forward pass. input data를 전달하여 output을 계산합니다.\n",
    "pred = linear(x) # wTx + b\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# Backward pass. 미분 계산\n",
    "loss.backward() # 중요한 점은 loss가 계산되기까지의, loss에 연결된 모든 tensor들의 gradient를 계산하는 것일뿐 실제로 가중치를 수정하지는 않습니다. 즉, gradient를 계산하여 weight.grad에 누적합니다.\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) # backward()에 의한 gradient는 weight.grad에 누적, 저장됩니다.\n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "optimizer.step() # 실제 가중치 수정은 여기서 수행됩니다. \n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item()) # loss가 감소한 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지 기본적인 학습 과정을 살펴보았습니다. <br/>\n",
    "\n",
    "이제 간단한 logistic regression 모델을 정의해서 위와 동일한 절차의 학습과정을 구현해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000], Loss: 0.6754\n",
      "Epoch [10/1000], Loss: 0.6546\n",
      "Epoch [15/1000], Loss: 0.6377\n",
      "Epoch [20/1000], Loss: 0.6237\n",
      "Epoch [25/1000], Loss: 0.6118\n",
      "Epoch [30/1000], Loss: 0.6016\n",
      "Epoch [35/1000], Loss: 0.5927\n",
      "Epoch [40/1000], Loss: 0.5849\n",
      "Epoch [45/1000], Loss: 0.5779\n",
      "Epoch [50/1000], Loss: 0.5715\n",
      "Epoch [55/1000], Loss: 0.5657\n",
      "Epoch [60/1000], Loss: 0.5604\n",
      "Epoch [65/1000], Loss: 0.5554\n",
      "Epoch [70/1000], Loss: 0.5508\n",
      "Epoch [75/1000], Loss: 0.5464\n",
      "Epoch [80/1000], Loss: 0.5422\n",
      "Epoch [85/1000], Loss: 0.5383\n",
      "Epoch [90/1000], Loss: 0.5345\n",
      "Epoch [95/1000], Loss: 0.5309\n",
      "Epoch [100/1000], Loss: 0.5274\n",
      "Epoch [105/1000], Loss: 0.5241\n",
      "Epoch [110/1000], Loss: 0.5209\n",
      "Epoch [115/1000], Loss: 0.5178\n",
      "Epoch [120/1000], Loss: 0.5148\n",
      "Epoch [125/1000], Loss: 0.5119\n",
      "Epoch [130/1000], Loss: 0.5090\n",
      "Epoch [135/1000], Loss: 0.5063\n",
      "Epoch [140/1000], Loss: 0.5037\n",
      "Epoch [145/1000], Loss: 0.5011\n",
      "Epoch [150/1000], Loss: 0.4986\n",
      "Epoch [155/1000], Loss: 0.4962\n",
      "Epoch [160/1000], Loss: 0.4939\n",
      "Epoch [165/1000], Loss: 0.4916\n",
      "Epoch [170/1000], Loss: 0.4894\n",
      "Epoch [175/1000], Loss: 0.4872\n",
      "Epoch [180/1000], Loss: 0.4851\n",
      "Epoch [185/1000], Loss: 0.4830\n",
      "Epoch [190/1000], Loss: 0.4811\n",
      "Epoch [195/1000], Loss: 0.4791\n",
      "Epoch [200/1000], Loss: 0.4772\n",
      "Epoch [205/1000], Loss: 0.4754\n",
      "Epoch [210/1000], Loss: 0.4736\n",
      "Epoch [215/1000], Loss: 0.4718\n",
      "Epoch [220/1000], Loss: 0.4701\n",
      "Epoch [225/1000], Loss: 0.4684\n",
      "Epoch [230/1000], Loss: 0.4668\n",
      "Epoch [235/1000], Loss: 0.4652\n",
      "Epoch [240/1000], Loss: 0.4636\n",
      "Epoch [245/1000], Loss: 0.4621\n",
      "Epoch [250/1000], Loss: 0.4605\n",
      "Epoch [255/1000], Loss: 0.4591\n",
      "Epoch [260/1000], Loss: 0.4576\n",
      "Epoch [265/1000], Loss: 0.4562\n",
      "Epoch [270/1000], Loss: 0.4548\n",
      "Epoch [275/1000], Loss: 0.4535\n",
      "Epoch [280/1000], Loss: 0.4521\n",
      "Epoch [285/1000], Loss: 0.4508\n",
      "Epoch [290/1000], Loss: 0.4495\n",
      "Epoch [295/1000], Loss: 0.4482\n",
      "Epoch [300/1000], Loss: 0.4470\n",
      "Epoch [305/1000], Loss: 0.4457\n",
      "Epoch [310/1000], Loss: 0.4445\n",
      "Epoch [315/1000], Loss: 0.4433\n",
      "Epoch [320/1000], Loss: 0.4421\n",
      "Epoch [325/1000], Loss: 0.4410\n",
      "Epoch [330/1000], Loss: 0.4398\n",
      "Epoch [335/1000], Loss: 0.4387\n",
      "Epoch [340/1000], Loss: 0.4376\n",
      "Epoch [345/1000], Loss: 0.4365\n",
      "Epoch [350/1000], Loss: 0.4354\n",
      "Epoch [355/1000], Loss: 0.4343\n",
      "Epoch [360/1000], Loss: 0.4332\n",
      "Epoch [365/1000], Loss: 0.4322\n",
      "Epoch [370/1000], Loss: 0.4311\n",
      "Epoch [375/1000], Loss: 0.4301\n",
      "Epoch [380/1000], Loss: 0.4291\n",
      "Epoch [385/1000], Loss: 0.4281\n",
      "Epoch [390/1000], Loss: 0.4271\n",
      "Epoch [395/1000], Loss: 0.4261\n",
      "Epoch [400/1000], Loss: 0.4251\n",
      "Epoch [405/1000], Loss: 0.4241\n",
      "Epoch [410/1000], Loss: 0.4232\n",
      "Epoch [415/1000], Loss: 0.4222\n",
      "Epoch [420/1000], Loss: 0.4212\n",
      "Epoch [425/1000], Loss: 0.4203\n",
      "Epoch [430/1000], Loss: 0.4193\n",
      "Epoch [435/1000], Loss: 0.4184\n",
      "Epoch [440/1000], Loss: 0.4175\n",
      "Epoch [445/1000], Loss: 0.4166\n",
      "Epoch [450/1000], Loss: 0.4156\n",
      "Epoch [455/1000], Loss: 0.4147\n",
      "Epoch [460/1000], Loss: 0.4138\n",
      "Epoch [465/1000], Loss: 0.4129\n",
      "Epoch [470/1000], Loss: 0.4120\n",
      "Epoch [475/1000], Loss: 0.4111\n",
      "Epoch [480/1000], Loss: 0.4102\n",
      "Epoch [485/1000], Loss: 0.4093\n",
      "Epoch [490/1000], Loss: 0.4084\n",
      "Epoch [495/1000], Loss: 0.4076\n",
      "Epoch [500/1000], Loss: 0.4067\n",
      "Epoch [505/1000], Loss: 0.4058\n",
      "Epoch [510/1000], Loss: 0.4049\n",
      "Epoch [515/1000], Loss: 0.4041\n",
      "Epoch [520/1000], Loss: 0.4032\n",
      "Epoch [525/1000], Loss: 0.4023\n",
      "Epoch [530/1000], Loss: 0.4015\n",
      "Epoch [535/1000], Loss: 0.4006\n",
      "Epoch [540/1000], Loss: 0.3998\n",
      "Epoch [545/1000], Loss: 0.3989\n",
      "Epoch [550/1000], Loss: 0.3981\n",
      "Epoch [555/1000], Loss: 0.3972\n",
      "Epoch [560/1000], Loss: 0.3964\n",
      "Epoch [565/1000], Loss: 0.3955\n",
      "Epoch [570/1000], Loss: 0.3947\n",
      "Epoch [575/1000], Loss: 0.3938\n",
      "Epoch [580/1000], Loss: 0.3930\n",
      "Epoch [585/1000], Loss: 0.3922\n",
      "Epoch [590/1000], Loss: 0.3913\n",
      "Epoch [595/1000], Loss: 0.3905\n",
      "Epoch [600/1000], Loss: 0.3896\n",
      "Epoch [605/1000], Loss: 0.3888\n",
      "Epoch [610/1000], Loss: 0.3880\n",
      "Epoch [615/1000], Loss: 0.3871\n",
      "Epoch [620/1000], Loss: 0.3863\n",
      "Epoch [625/1000], Loss: 0.3855\n",
      "Epoch [630/1000], Loss: 0.3846\n",
      "Epoch [635/1000], Loss: 0.3838\n",
      "Epoch [640/1000], Loss: 0.3830\n",
      "Epoch [645/1000], Loss: 0.3822\n",
      "Epoch [650/1000], Loss: 0.3813\n",
      "Epoch [655/1000], Loss: 0.3805\n",
      "Epoch [660/1000], Loss: 0.3797\n",
      "Epoch [665/1000], Loss: 0.3789\n",
      "Epoch [670/1000], Loss: 0.3780\n",
      "Epoch [675/1000], Loss: 0.3772\n",
      "Epoch [680/1000], Loss: 0.3764\n",
      "Epoch [685/1000], Loss: 0.3755\n",
      "Epoch [690/1000], Loss: 0.3747\n",
      "Epoch [695/1000], Loss: 0.3739\n",
      "Epoch [700/1000], Loss: 0.3731\n",
      "Epoch [705/1000], Loss: 0.3722\n",
      "Epoch [710/1000], Loss: 0.3714\n",
      "Epoch [715/1000], Loss: 0.3706\n",
      "Epoch [720/1000], Loss: 0.3698\n",
      "Epoch [725/1000], Loss: 0.3690\n",
      "Epoch [730/1000], Loss: 0.3681\n",
      "Epoch [735/1000], Loss: 0.3673\n",
      "Epoch [740/1000], Loss: 0.3665\n",
      "Epoch [745/1000], Loss: 0.3657\n",
      "Epoch [750/1000], Loss: 0.3648\n",
      "Epoch [755/1000], Loss: 0.3640\n",
      "Epoch [760/1000], Loss: 0.3632\n",
      "Epoch [765/1000], Loss: 0.3624\n",
      "Epoch [770/1000], Loss: 0.3615\n",
      "Epoch [775/1000], Loss: 0.3607\n",
      "Epoch [780/1000], Loss: 0.3599\n",
      "Epoch [785/1000], Loss: 0.3591\n",
      "Epoch [790/1000], Loss: 0.3582\n",
      "Epoch [795/1000], Loss: 0.3574\n",
      "Epoch [800/1000], Loss: 0.3566\n",
      "Epoch [805/1000], Loss: 0.3557\n",
      "Epoch [810/1000], Loss: 0.3549\n",
      "Epoch [815/1000], Loss: 0.3541\n",
      "Epoch [820/1000], Loss: 0.3533\n",
      "Epoch [825/1000], Loss: 0.3524\n",
      "Epoch [830/1000], Loss: 0.3516\n",
      "Epoch [835/1000], Loss: 0.3508\n",
      "Epoch [840/1000], Loss: 0.3499\n",
      "Epoch [845/1000], Loss: 0.3491\n",
      "Epoch [850/1000], Loss: 0.3483\n",
      "Epoch [855/1000], Loss: 0.3474\n",
      "Epoch [860/1000], Loss: 0.3466\n",
      "Epoch [865/1000], Loss: 0.3458\n",
      "Epoch [870/1000], Loss: 0.3449\n",
      "Epoch [875/1000], Loss: 0.3441\n",
      "Epoch [880/1000], Loss: 0.3433\n",
      "Epoch [885/1000], Loss: 0.3424\n",
      "Epoch [890/1000], Loss: 0.3416\n",
      "Epoch [895/1000], Loss: 0.3408\n",
      "Epoch [900/1000], Loss: 0.3399\n",
      "Epoch [905/1000], Loss: 0.3391\n",
      "Epoch [910/1000], Loss: 0.3383\n",
      "Epoch [915/1000], Loss: 0.3374\n",
      "Epoch [920/1000], Loss: 0.3366\n",
      "Epoch [925/1000], Loss: 0.3358\n",
      "Epoch [930/1000], Loss: 0.3349\n",
      "Epoch [935/1000], Loss: 0.3341\n",
      "Epoch [940/1000], Loss: 0.3332\n",
      "Epoch [945/1000], Loss: 0.3324\n",
      "Epoch [950/1000], Loss: 0.3316\n",
      "Epoch [955/1000], Loss: 0.3307\n",
      "Epoch [960/1000], Loss: 0.3299\n",
      "Epoch [965/1000], Loss: 0.3290\n",
      "Epoch [970/1000], Loss: 0.3282\n",
      "Epoch [975/1000], Loss: 0.3273\n",
      "Epoch [980/1000], Loss: 0.3265\n",
      "Epoch [985/1000], Loss: 0.3257\n",
      "Epoch [990/1000], Loss: 0.3248\n",
      "Epoch [995/1000], Loss: 0.3240\n",
      "Epoch [1000/1000], Loss: 0.3231\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np # Python에선 일반적으로 데이터셋으로부터 데이터를 가져올때 numpy의 ndarray 클래스를 사용합니다. \n",
    "                   # 따라서 numpy의 ndarray로부터 torch의 tensor로 변환하는 과정이 필연적임을 기억해야합니다.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Hyper-parameters (사용자가 수정 가능한 파라미터)\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Toy dataset.\n",
    "x_data = np.array([[1, 2],[2, 3],[3, 1],[4, 3],[5, 3],[6, 2]])\n",
    "y_data = np.array([[0],[0],[0],[1],[1],[1]])\n",
    "\n",
    "# Define your network\n",
    "class MyModelNet(nn.Module):\n",
    "    def __init__(self, inp_size, out_size):\n",
    "        super(MyModelNet, self).__init__()\n",
    "        self.lin1 = nn.Linear(inp_size, 3) # w1: (2, 3), b1: (3)\n",
    "        self.lin2 = nn.Linear(3, out_size) # w2: (3, 1), b2: (1)\n",
    "        self.sigmoid = nn.Sigmoid() # activation function.\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.lin2(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Logistic regression model\n",
    "model = MyModelNet(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    input = torch.from_numpy(x_data).float()\n",
    "    target = torch.from_numpy(y_data).float()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad() # zero_grad()는 누적된 gradient, 곧 weight.grad, bias.grad에 저장된 값을 초기화합니다. \n",
    "                            # backward()를 통해 계산된 gradient가 weight.grad에 새로 덮어씌워지는 것이 아닌 누적되는 것이기 때문에\n",
    "                            # 특별한 이유가 없다면 초기화해야합니다. \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "\n",
    "# Plot the graph. 예측한 값이 Y 데이터를 잘 표현하고 있는 것을 확인할 수 있습니다.\n",
    "predicted = model(torch.from_numpy(x_data).float()).detach().numpy() # 모델 예측 결과(tensor)를 다시 numpy로 변환\n",
    "plt.plot(y_data, 'ro', label='Original data')\n",
    "plt.plot(predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt15",
   "language": "python",
   "name": "pt15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}