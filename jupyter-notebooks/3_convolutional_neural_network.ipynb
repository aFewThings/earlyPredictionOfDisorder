{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "references: \n",
    "- [https://wegonnamakeit.tistory.com/48](https://wegonnamakeit.tistory.com/48)\n",
    "- [http://taewan.kim/post/cnn/](http://taewan.kim/post/cnn/)\n",
    "- [https://cheong.netlify.app/machine-learning/2019-10-14---cs231n-cnn-architectures/](https://cheong.netlify.app/machine-learning/2019-10-14---cs231n-cnn-architectures/)\n",
    "- [https://kjhov195.github.io/2020-01-07-activation_function_2/](https://kjhov195.github.io/2020-01-07-activation_function_2/)\n",
    "- [https://jsideas.net/batch_normalization/](https://jsideas.net/batch_normalization/)\n",
    "\n",
    "### Basic CNN\n",
    "CNN을 구성할때 보편적으로 사용되는 layers에 대해서 알아보겠습니다. <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution layer (or spatial convolution layer)\n",
    "\n",
    "![img](http://deeplearning.net/software/theano/_images/numerical_padding_strides.gif)\n",
    "\n",
    "- Convolution: 이미지 위에 stride 값 만큼 filter(or kernel)을 이동시키면서 겹쳐지는 부분의 각 원소의 값을 모두 곱한 뒤 합산한 값을 출력하는 연산\n",
    "- filter(or kernel): number_of_filters x input_channels x kernel_size x kernel_size\n",
    "- Stride: filter를 sliding window 방식으로 한 번에 이동시키는 간격\n",
    "- Padding: pad 크기 만큼 이미지의 상하좌우에 '0'으로 값을 채우는 것. output의 width, height 크기를 조절하기 위해 사용합니다. \n",
    "- input(image or features): Batch x Channel x Height x Width (Pytorch: BCHW format, Tensorflow: BHWC format)\n",
    "  - Batch는 입력데이터의 묶음을 의미합니다. 입력데이터를 이미지 한 장으로 구성한다면 1 x C x H x W와 같습니다. 이미지의 크기가 256 x 256이고, RGB channel이라면 입력데이터는 1x3x256x256 입니다.\n",
    "- output(features or feature map): Batch x number_of_filters x computed_height x computed_width\n",
    "  - computed_width = ((width - kernel_size + 2*pad) / stride) + 1\n",
    "  - computed_height = ((height - kernel_size + 2*pad) / stride) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv.weight:\n",
      " Parameter containing:\n",
      "tensor([[[[ 0.0551, -0.0760,  0.1334],\n",
      "          [-0.1192, -0.0871,  0.1630],\n",
      "          [-0.1401, -0.0445,  0.1765]],\n",
      "\n",
      "         [[-0.0620, -0.1905,  0.0928],\n",
      "          [-0.0963, -0.1175, -0.1028],\n",
      "          [ 0.1328,  0.1005,  0.0581]],\n",
      "\n",
      "         [[-0.0466, -0.0871,  0.0760],\n",
      "          [ 0.1106, -0.0890, -0.1355],\n",
      "          [-0.0618,  0.0945,  0.0847]]],\n",
      "\n",
      "\n",
      "        [[[-0.0892,  0.0151, -0.1040],\n",
      "          [ 0.1507, -0.0944,  0.0347],\n",
      "          [-0.1797, -0.0250, -0.1183]],\n",
      "\n",
      "         [[ 0.1784,  0.0276, -0.1322],\n",
      "          [ 0.1316, -0.0685, -0.0231],\n",
      "          [ 0.1436,  0.1272, -0.1201]],\n",
      "\n",
      "         [[ 0.1577,  0.0682, -0.1657],\n",
      "          [-0.1063,  0.1061,  0.1656],\n",
      "          [-0.0721, -0.0264,  0.1091]]]], requires_grad=True)\n",
      "conv.weight.shape:\n",
      " torch.Size([2, 3, 3, 3])\n",
      "\n",
      "conv.bias:\n",
      " Parameter containing:\n",
      "tensor([0.1884, 0.0524], requires_grad=True)\n",
      "conv.bias.shape:\n",
      " torch.Size([2])\n",
      "\n",
      "input:\n",
      " tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
      "output:\n",
      " tensor([[[[ 0.2893,  0.1153,  0.1153,  ...,  0.1153,  0.1153,  0.1153],\n",
      "          [ 0.2378,  0.0103,  0.0103,  ...,  0.0103,  0.0103,  0.0103],\n",
      "          [ 0.2378,  0.0103,  0.0103,  ...,  0.0103,  0.0103,  0.0103],\n",
      "          ...,\n",
      "          [ 0.2378,  0.0103,  0.0103,  ...,  0.0103,  0.0103,  0.0103],\n",
      "          [ 0.2378,  0.0103,  0.0103,  ...,  0.0103,  0.0103,  0.0103],\n",
      "          [ 0.2378,  0.0103,  0.0103,  ...,  0.0103,  0.0103,  0.0103]],\n",
      "\n",
      "         [[ 0.1193,  0.1871,  0.1871,  ...,  0.1871,  0.1871,  0.1871],\n",
      "          [-0.1718,  0.1430,  0.1430,  ...,  0.1430,  0.1430,  0.1430],\n",
      "          [-0.1718,  0.1430,  0.1430,  ...,  0.1430,  0.1430,  0.1430],\n",
      "          ...,\n",
      "          [-0.1718,  0.1430,  0.1430,  ...,  0.1430,  0.1430,  0.1430],\n",
      "          [-0.1718,  0.1430,  0.1430,  ...,  0.1430,  0.1430,  0.1430],\n",
      "          [-0.1718,  0.1430,  0.1430,  ...,  0.1430,  0.1430,  0.1430]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n",
      "output.shape:\n",
      " torch.Size([1, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 2 # out_channels은 number_of_filters과 동일하다.\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "pad = 1\n",
    "\n",
    "# Basic 2D convolution layer\n",
    "conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=pad)\n",
    "\n",
    "print('conv.weight:\\n', conv.weight) \n",
    "print('conv.weight.shape:\\n', conv.weight.shape) # filter(kernel) size는 곧 weights size와 동일합니다.\n",
    "print()\n",
    "print('conv.bias:\\n', conv.bias)\n",
    "print('conv.bias.shape:\\n', conv.bias.shape) # Convolution layer도 linear layer와 마찬가지로 bias를 가질 수 있습니다.\n",
    "print()\n",
    "\n",
    "inp = torch.ones((1, 3, 256, 256)) # 256x256x3\n",
    "print('input:\\n', inp)\n",
    "\n",
    "out = conv(inp)\n",
    "print('output:\\n', out)\n",
    "print('output.shape:\\n', out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Pooling layer\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2017/07/Screen-Shot-2017-07-27-at-11.43.19-AM.png\" width=\"600\">\n",
    "\n",
    "- pooling_size(kernel_size): pooling layer에서 사용되는 sliding window size.\n",
    "- stride: sliding window가 이동하는 간격 (보통 pooling size와 동일하게 설정함)\n",
    "- input: Batch x Channel x Height x Width\n",
    "- output: Batch x Channel x computed_height x computed_width (Channel은 유지함)\n",
    "- computed_height = Height / pooling_size\n",
    "- computed_width = Width / pooling_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4049, 0.7216, 0.5804, 0.7205],\n",
      "          [0.0937, 0.6157, 0.9569, 0.0500],\n",
      "          [0.7333, 0.6868, 0.8871, 0.8713],\n",
      "          [0.1976, 0.2960, 0.5173, 0.7368]]]])\n",
      "torch.Size([1, 1, 4, 4])\n",
      "tensor([[[[0.7216, 0.9569],\n",
      "          [0.7333, 0.8871]]]])\n",
      "torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Max pooling layer\n",
    "maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "inp = torch.rand((1, 1, 4, 4)) # BxCxHxW\n",
    "\n",
    "out = maxpool(inp)\n",
    "\n",
    "print(inp)\n",
    "print(inp.shape)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "\n",
    "<img src=\"https://kjhov195.github.io/post_img/200107/image11.png\" width=\"600\">\n",
    "\n",
    "이미지 처리 분야에선 일반적인 경우에 모델이 깊을수록 좋은 성능을 나타냅니다.<br/>\n",
    "하지만, layer가 쌓임에 따라 고질적인 Vanishing gradient 현상이 나타나므로 적절한 activation function을 적용하는 것이 중요합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1., -1., -1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([0.7311, 0.7311, 0.5000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inp = torch.ones((4))\n",
    "inp[2:4] = -1 # 2, 3 index 값을 -1로 바꾼다.\n",
    "\n",
    "relu = nn.ReLU() # activation function\n",
    "sigmoid  = nn.Sigmoid()\n",
    "\n",
    "out1 = relu(inp)\n",
    "out2 = sigmoid(out1)\n",
    "\n",
    "print(inp)\n",
    "print(out1)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "|![](https://image.slidesharecdn.com/dlmmdcud1l06optimization-170427160940/95/optimizing-deep-networks-d1l6-insightdcu-machine-learning-workshop-2017-8-638.jpg?cb=1493309658)|\n",
    "|:--:|\n",
    "|Intenal Covariate Shift Problem|\n",
    "\n",
    "|![](https://guillaumebrg.files.wordpress.com/2016/02/bn.png?w=656)|\n",
    "|:--:|\n",
    "|Batch Normalization (BN)|\n",
    "\n",
    "<br/>\n",
    "batch normalization은 입력 데이터를 normalize한 뒤, affine 변환을 적용하는 것으로, layer가 깊어질수록 입력 데이터의 distribution이 달라지는 문제를 해결하고자 사용됩니다. <br/>\n",
    "batch normalization은 다음과 같은 장점을 가집니다.\n",
    "\n",
    "- 안정되고 빠른 훈련\n",
    "- Vanishing gradient, exploding gradient 억제\n",
    "- 약간의 regularization 효과\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5090,  1.5601],\n",
      "          [ 0.8618,  0.0343]],\n",
      "\n",
      "         [[-0.1874,  0.3379],\n",
      "          [ 0.1858,  1.5325]],\n",
      "\n",
      "         [[ 0.1149,  0.7461],\n",
      "          [-0.1042,  0.5804]]]])\n",
      "tensor([0.0487, 0.0467, 0.0334])\n",
      "tensor([0.9830, 0.9553, 0.9157])\n",
      "tensor([[[[-1.2624,  1.3606],\n",
      "          [ 0.4754, -0.5736]],\n",
      "\n",
      "         [[-1.0163, -0.2008],\n",
      "          [-0.4369,  1.6540]],\n",
      "\n",
      "         [[-0.6396,  1.2006],\n",
      "          [-1.2784,  0.7174]]]], grad_fn=<NativeBatchNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inp = torch.randn((1, 3, 2, 2))\n",
    "bn = nn.BatchNorm2d(num_features=3)\n",
    "\n",
    "out = bn(inp)\n",
    "\n",
    "print(inp)\n",
    "\n",
    "print(bn.running_mean)\n",
    "print(bn.running_var)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training simple CNN\n",
    "\n",
    "Multiple layers를 한 단위로 엮은 개념을 building block이라고 합니다. <br/>\n",
    "지금까지 소개한 layers를 building block으로 구성하여 간단한 CNN 모델을 정의해보겠습니다. <br/>\n",
    "그리고 이 모델을 torchvision 패키지가 제공하는 MNIST 필기체 이미지 데이터셋을 사용해서 훈련시켜보겠습니다.\n",
    "\n",
    "MNIST 데이터셋은 다음과 같은 이미지들을  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1798\n",
      "Epoch [1/5], Step [200/600], Loss: 0.0870\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0917\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0498\n",
      "Epoch [1/5], Step [500/600], Loss: 0.0759\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0515\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0700\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0225\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0363\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0477\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0130\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0520\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0129\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0790\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0091\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0250\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0245\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0223\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0403\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0479\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0552\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0033\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0149\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0082\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0510\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0183\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0470\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0085\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0238\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0051\n",
      "Test Accuracy of the model on the 10000 test images: 98.99 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5   # 데이터 전체를 훈련하는 횟수\n",
    "num_classes = 10 # 숫자의 종류 (0, 1, 2, ..., 9)\n",
    "batch_size = 100 # 입력 데이터 묶음\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes) # fully connected layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Architecture\n",
    "\n",
    "이외에도 좋은 성능을 내는 모델들은 다음과 같은 방법들을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1x1 Convolution layer (convolution with kernel_size = 1)\n",
    "\n",
    "![https://cheong.netlify.app/static/fb817ed940cd331991e5f40effdaf455/799d3/image16.png](https://cheong.netlify.app/static/fb817ed940cd331991e5f40effdaf455/799d3/image16.png)\n",
    "\n",
    "일반적으로 input의 channels을 줄여 연산량을 줄이거나, 다른 features의 channels와 개수를 일치시키고 싶을때 사용됩니다. <br/>\n",
    "예를 들어 3x3 conv, 5x5 conv와 같이 연산량이 큰 컨볼루션을 적용하기 전에 1x1 conv로 입력 차원을 줄인뒤 3x3 conv를 적용하면 더 적은 비용으로 비슷한 효과를 볼 수 있습니다. (ResNet의 Bottleneck block) <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 4, 4])\n",
      "torch.Size([1, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inp = torch.ones((1, 10, 4, 4)) # C=10\n",
    "\n",
    "conv1x1 = nn.Conv2d(in_channels=10, out_channels=3, kernel_size=1, stride=1, padding=0) # kernel_size = 1\n",
    "\n",
    "out = conv1x1(inp)\n",
    "\n",
    "print(inp.shape)\n",
    "print(out.shape) # C=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Element-wise addition / Concatenate\n",
    "\n",
    "|![](https://codeforwin.org/ezoimgfmt/secureservercdn.net/160.153.138.219/b79.d22.myftpupload.com/wp-content/uploads/2015/07/matrix-addition.png?ezimgfmt=rs:392x204/rscb1)|\n",
    "|:--:|\n",
    "|Element-wise Addition|\n",
    "\n",
    "|![](https://lh3.googleusercontent.com/proxy/FzrxubMd4t113IigFibyfUm283qNi3_ZxCGzMaMw9Rwj6w2SmhtWKtHefLTk7XMpZmM9EJfoE1CLUE5PRYKMv2gDImWTY-1qABadiHp-e-ukUux8h8axxX_LbeBUI0QXTD1nHNRA_AbHc1OiWnLDyxOl9SpXwAo)|\n",
    "|:--:|\n",
    "|Concatenate (Inception module)|\n",
    "\n",
    "element-wise addition은 말 그대로 동일한 차원을 가진 tensor의 요소끼리 더하는 연산을 의미합니다. <br/>\n",
    "concatenate은 선택한 차원을 기준으로 tensors를 연결하는 것을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[9, 8, 7],\n",
      "        [6, 5, 4],\n",
      "        [3, 2, 1]])\n",
      "tensor([[10, 10, 10],\n",
      "        [10, 10, 10],\n",
      "        [10, 10, 10]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9],\n",
      "        [9, 8, 7],\n",
      "        [6, 5, 4],\n",
      "        [3, 2, 1]])\n",
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "A = torch.arange(1, 10).view(3, 3)\n",
    "B = torch.arange(9, 0, -1).view(3, 3)\n",
    "\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "out1 = A + B # element-wise addition\n",
    "print(out1)\n",
    "\n",
    "out2 = torch.cat((A, B), 0) # concat in 0 dim\n",
    "print(out2)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip connection (shortcut connection)\n",
    "\n",
    "|![](https://datascienceschool.net/upfiles/6182312059774a81a2a26246bd4e83f2.png)|\n",
    "|:---:|\n",
    "|*Skip Connection (ResNet)*|\n",
    "\n",
    "|![](https://cheong.netlify.app/static/5a711b3b3b3d4789e4d0e0fc742c0e11/7f576/image24.png)|\n",
    "|:---:|\n",
    "|*Bottleneck Block (ResNet)*|\n",
    "<br/>\n",
    "\n",
    "skip connection은 ResNet에서 제안되어졌으며 몇가지 장점들로 인해 많은 CNNs에서 필수적인 요소로 사용하고 있습니다. <br/>\n",
    "\n",
    "- 깊은 모델의 Degradation 현상 해소\n",
    "- 원활한 gradient 전파\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([1, 256, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_in = torch.ones((1, 256, 28, 28))\n",
    "\n",
    "conv1 = nn.Conv2d(256, 64, kernel_size=1, stride=1, padding=0) # 1x1 conv\n",
    "conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1) # 3x3 conv with padding\n",
    "conv3 = nn.Conv2d(64, 256, kernel_size=1, stride=1, padding=0) # 1x1 conv\n",
    "F = nn.Sequential(conv1, conv2, conv3) # Bottleneck block\n",
    "\n",
    "F_out = F(x_in)\n",
    "# nn.Sequential 모듈을 사용하면 아래와 같이 순서대로 feedforward 합니다.\n",
    "# out = conv1(x_in)\n",
    "# out = conv2(out)\n",
    "# out = conv3(out)\n",
    "\n",
    "print(F_out.shape)\n",
    "\n",
    "x_out = F_out + x_in # skip connection (element-wise addition)\n",
    "print(x_out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt15",
   "language": "python",
   "name": "pt15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
