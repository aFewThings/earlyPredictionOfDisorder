{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "references: \n",
    "- [https://wegonnamakeit.tistory.com/48](https://wegonnamakeit.tistory.com/48)\n",
    "- [http://taewan.kim/post/cnn/](http://taewan.kim/post/cnn/)\n",
    "- [https://cheong.netlify.app/machine-learning/2019-10-14---cs231n-cnn-architectures/](https://cheong.netlify.app/machine-learning/2019-10-14---cs231n-cnn-architectures/)\n",
    "- [https://kjhov195.github.io/2020-01-07-activation_function_2/](https://kjhov195.github.io/2020-01-07-activation_function_2/)\n",
    "- [https://jsideas.net/batch_normalization/](https://jsideas.net/batch_normalization/)\n",
    "\n",
    "### Basic CNN\n",
    "CNN을 구성할때 보편적으로 사용되는 layers에 대해서 알아보겠습니다. <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution layer (or spatial convolution layer)\n",
    "\n",
    "![img](http://deeplearning.net/software/theano/_images/numerical_padding_strides.gif)\n",
    "\n",
    "- Convolution: 이미지 위에 stride 값 만큼 filter(or kernel)을 이동시키면서 겹쳐지는 부분의 각 원소의 값을 모두 곱한 뒤 합산한 값을 출력하는 연산\n",
    "- filter(or kernel): number_of_filters x input_channels x kernel_size x kernel_size\n",
    "- Stride: filter를 sliding window 방식으로 한 번에 이동시키는 간격\n",
    "- Padding: pad 크기 만큼 이미지의 상하좌우에 '0'으로 값을 채우는 것. output의 width, height 크기를 조절하기 위해 사용합니다. \n",
    "- input(image or features): Batch x Channel x Height x Width (Pytorch: BCHW format, Tensorflow: BHWC format)\n",
    "  - Batch는 입력데이터의 묶음을 의미합니다. 입력데이터를 이미지 한 장으로 구성한다면 1 x C x H x W와 같습니다. 이미지의 크기가 256 x 256이고, RGB channel이라면 입력데이터는 1x3x256x256 입니다.\n",
    "- output(features or feature map): Batch x number_of_filters x computed_height x computed_width\n",
    "  - computed_width = ((width - kernel_size + 2*pad) / stride) + 1\n",
    "  - computed_height = ((height - kernel_size + 2*pad) / stride) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv.weight:\n",
      " Parameter containing:\n",
      "tensor([[[[-0.1714, -0.1633,  0.0155],\n",
      "          [ 0.0130,  0.0025, -0.0413],\n",
      "          [-0.1491, -0.0082, -0.1326]],\n",
      "\n",
      "         [[ 0.0055,  0.1410, -0.1737],\n",
      "          [ 0.0885, -0.0911, -0.1266],\n",
      "          [-0.0893, -0.0882,  0.1075]],\n",
      "\n",
      "         [[ 0.0076,  0.1829,  0.0499],\n",
      "          [-0.0746, -0.0210, -0.1447],\n",
      "          [-0.0191, -0.0113, -0.1284]]],\n",
      "\n",
      "\n",
      "        [[[-0.1407, -0.0149, -0.0173],\n",
      "          [-0.0711,  0.0406, -0.1440],\n",
      "          [-0.1912,  0.1816,  0.1623]],\n",
      "\n",
      "         [[ 0.0691,  0.1679, -0.0347],\n",
      "          [ 0.1207,  0.0355,  0.1410],\n",
      "          [ 0.1236, -0.0073,  0.1920]],\n",
      "\n",
      "         [[ 0.1074,  0.1647,  0.1904],\n",
      "          [ 0.0600, -0.1610,  0.0631],\n",
      "          [ 0.1410,  0.1800, -0.1080]]]], requires_grad=True)\n",
      "conv.weight.shape:\n",
      " torch.Size([2, 3, 3, 3])\n",
      "\n",
      "conv.bias:\n",
      " Parameter containing:\n",
      "tensor([-0.1840,  0.1339], requires_grad=True)\n",
      "conv.bias.shape:\n",
      " torch.Size([2])\n",
      "\n",
      "input:\n",
      " tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
      "output:\n",
      " tensor([[[[-0.8674, -1.0980, -1.0980,  ..., -1.0980, -1.0980, -1.0980],\n",
      "          [-0.8152, -1.2040, -1.2040,  ..., -1.2040, -1.2040, -1.2040],\n",
      "          [-0.8152, -1.2040, -1.2040,  ..., -1.2040, -1.2040, -1.2040],\n",
      "          ...,\n",
      "          [-0.8152, -1.2040, -1.2040,  ..., -1.2040, -1.2040, -1.2040],\n",
      "          [-0.8152, -1.2040, -1.2040,  ..., -1.2040, -1.2040, -1.2040],\n",
      "          [-0.8152, -1.2040, -1.2040,  ..., -1.2040, -1.2040, -1.2040]],\n",
      "\n",
      "         [[ 0.7097,  0.8927,  0.8927,  ...,  0.8927,  0.8927,  0.8927],\n",
      "          [ 1.1658,  1.3847,  1.3847,  ...,  1.3847,  1.3847,  1.3847],\n",
      "          [ 1.1658,  1.3847,  1.3847,  ...,  1.3847,  1.3847,  1.3847],\n",
      "          ...,\n",
      "          [ 1.1658,  1.3847,  1.3847,  ...,  1.3847,  1.3847,  1.3847],\n",
      "          [ 1.1658,  1.3847,  1.3847,  ...,  1.3847,  1.3847,  1.3847],\n",
      "          [ 1.1658,  1.3847,  1.3847,  ...,  1.3847,  1.3847,  1.3847]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n",
      "output.shape:\n",
      " torch.Size([1, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 2 # out_channels은 number_of_filters과 동일하다.\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "pad = 1\n",
    "\n",
    "# Basic 2D convolution layer\n",
    "conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=pad)\n",
    "\n",
    "print('conv.weight:\\n', conv.weight) \n",
    "print('conv.weight.shape:\\n', conv.weight.shape) # filter(kernel) size는 곧 weights size와 동일합니다.\n",
    "print()\n",
    "print('conv.bias:\\n', conv.bias)\n",
    "print('conv.bias.shape:\\n', conv.bias.shape) # Convolution layer도 linear layer와 마찬가지로 bias를 가질 수 있습니다.\n",
    "print()\n",
    "\n",
    "inp = torch.ones((1, 3, 256, 256)) # 256x256x3\n",
    "print('input:\\n', inp)\n",
    "\n",
    "out = conv(inp)\n",
    "print('output:\\n', out)\n",
    "print('output.shape:\\n', out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Pooling layer\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2017/07/Screen-Shot-2017-07-27-at-11.43.19-AM.png\" width=\"600\">\n",
    "\n",
    "- pooling_size(kernel_size): pooling layer에서 사용되는 sliding window size.\n",
    "- stride: sliding window가 이동하는 간격 (보통 pooling size와 동일하게 설정함)\n",
    "- input: Batch x Channel x Height x Width\n",
    "- output: Batch x Channel x computed_height x computed_width (Channel은 유지함)\n",
    "- computed_height = Height / pooling_size\n",
    "- computed_width = Width / pooling_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.9476, 0.7413, 0.5341, 0.5301],\n",
      "          [0.4611, 0.5025, 0.0708, 0.9261],\n",
      "          [0.8010, 0.7546, 0.5695, 0.5796],\n",
      "          [0.4325, 0.7409, 0.6367, 0.4468]]]])\n",
      "torch.Size([1, 1, 4, 4])\n",
      "tensor([[[[0.9476, 0.9261],\n",
      "          [0.8010, 0.6367]]]])\n",
      "torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Max pooling layer\n",
    "maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "inp = torch.rand((1, 1, 4, 4)) # BxCxHxW\n",
    "\n",
    "out = maxpool(inp)\n",
    "\n",
    "print(inp)\n",
    "print(inp.shape)\n",
    "print(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "\n",
    "<img src=\"https://kjhov195.github.io/post_img/200107/image11.png\" width=\"600\">\n",
    "\n",
    "이미지 처리 분야에선 일반적인 경우에 모델이 깊을수록 좋은 성능을 나타냅니다.<br/>\n",
    "하지만, layer가 쌓임에 따라 고질적인 Vanishing gradient 현상이 나타나므로 적절한 activation function을 적용하는 것이 중요합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1., -1., -1.])\n",
      "tensor([1., 1., 0., 0.])\n",
      "tensor([0.7311, 0.7311, 0.5000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.ones((4))\n",
    "inp[2:4] = -1 # 2, 3 index 값을 -1로 바꾼다.\n",
    "\n",
    "relu = nn.ReLU() # activation function\n",
    "sigmoid  = nn.Sigmoid()\n",
    "\n",
    "out1 = relu(inp)\n",
    "out2 = sigmoid(out1)\n",
    "\n",
    "print(inp)\n",
    "print(out1)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "|![](https://image.slidesharecdn.com/dlmmdcud1l06optimization-170427160940/95/optimizing-deep-networks-d1l6-insightdcu-machine-learning-workshop-2017-8-638.jpg?cb=1493309658)|\n",
    "|:--:|\n",
    "|Intenal Covariate Shift Problem|\n",
    "\n",
    "|![](https://guillaumebrg.files.wordpress.com/2016/02/bn.png?w=656)|\n",
    "|:--:|\n",
    "|Batch Normalization (BN)|\n",
    "\n",
    "<br/>\n",
    "batch normalization은 입력 데이터를 normalize한 뒤, affine 변환을 적용하는 것으로, layer가 깊어질수록 입력 데이터의 distribution이 달라지는 문제를 해결하고자 사용됩니다. <br/>\n",
    "batch normalization은 다음과 같은 장점을 가집니다.\n",
    "\n",
    "- 안정되고 빠른 훈련\n",
    "- Vanishing gradient, exploding gradient 억제\n",
    "- 약간의 regularization 효과\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.1914,  0.0297],\n",
      "          [ 0.4872, -0.6642]],\n",
      "\n",
      "         [[-0.2333,  1.1330],\n",
      "          [-0.2106,  0.3520]],\n",
      "\n",
      "         [[-0.6470,  1.0888],\n",
      "          [-0.8806, -0.6108]]]])\n",
      "tensor([-0.0085,  0.0260, -0.0262])\n",
      "tensor([0.9229, 0.9412, 0.9826])\n",
      "tensor([[[[-0.2576,  0.2760],\n",
      "          [ 1.3795, -1.3979]],\n",
      "\n",
      "         [[-0.8882,  1.5703],\n",
      "          [-0.8472,  0.1651]],\n",
      "\n",
      "         [[-0.4887,  1.7170],\n",
      "          [-0.7856, -0.4427]]]], grad_fn=<NativeBatchNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn((1, 3, 2, 2))\n",
    "bn = nn.BatchNorm2d(num_features=3)\n",
    "\n",
    "out = bn(inp)\n",
    "\n",
    "print(inp)\n",
    "\n",
    "print(bn.running_mean)\n",
    "print(bn.running_var)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training simple CNN\n",
    "\n",
    "Multiple layers를 한 단위로 엮은 개념을 building block이라고 합니다. <br/>\n",
    "지금까지 소개한 layers를 block으로 구성하여 간단한 CNN 모델을 정의해보겠습니다. <br/>\n",
    "그리고 이 모델을 torchvision 패키지가 제공하는 MNIST 필기체 이미지 데이터셋을 사용해서 훈련시켜보겠습니다.\n",
    "\n",
    "MNIST 데이터셋은 다음과 같이 0 ~ 9까지의 숫자 이미지들을 포함하고 있습니다. <br/>\n",
    "각 이미지는 28x28 크기, channel=1의 흑백 이미지입니다. \n",
    "\n",
    "![mnist](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F21792E37593A321515)\n",
    " \n",
    "아래 훈련과정은 torchvision 패키지로부터 데이터셋을 `./data/`에 다운받습니다.  \n",
    "다운 받은 데이터셋은 DataLoader에 등록하며, 반복적으로 batch_size만큼 이미지 데이터(images)와 이미지의 라벨(labels)을 가져오게 됩니다. <br/>\n",
    "여기서 라벨이란 이미지가 0~9까지의 숫자 중 무엇에 해당하는지 one-hot vector로 표현한 것을 의미합니다. <br/>\n",
    "라벨은 이후 모델이 이미지를 입력받아 어떤 클래스인지 구별(classify)할 때에 비교되는 정답 데이터로 사용됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1362\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1374\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0918\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1465\n",
      "Epoch [1/5], Step [500/600], Loss: 0.0440\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0440\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0094\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1048\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0209\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0136\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0279\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0336\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0441\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0052\n",
      "Epoch [3/5], Step [300/600], Loss: 0.1770\n",
      "Epoch [3/5], Step [400/600], Loss: 0.1389\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0618\n",
      "Epoch [3/5], Step [600/600], Loss: 0.1027\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0500\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0803\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0088\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0412\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0153\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0232\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0504\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0022\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0464\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0265\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0125\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0024\n",
      "Test Accuracy of the model on the 10000 test images: 98.98 %\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5   # 데이터 전체를 훈련하는 횟수\n",
    "num_classes = 10 # 숫자의 종류 (0, 1, 2, ..., 9)\n",
    "batch_size = 100 # 입력 데이터 묶음\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Convolutional neural network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # nn.Sequential은 등록된 modules을 순차적으로 실행시킵니다.  \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes) # fully connected layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "                                                    # x shape: Batch x 1 x 28 x 28\n",
    "        out = self.layer1(x)                    # out shape: Batch x 16 x 14 x 14\n",
    "        out = self.layer2(out)                 # out shape: Batch x 32 x 7 x 7\n",
    "        out = out.reshape(out.size(0), -1) # out shape: Batch x (32 * 7 * 7)\n",
    "        out = self.fc(out)                      # out shape: Batch x 10\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "\"\"\"\n",
    "nn.CrossEntropyLoss는 log_softmax와 NLLLoss가 함께 적용된 형태입니다. \n",
    "간단히 말해, 모델의 output에 activation function으로 softmax가 자동으로 적용되니 모델에 따로 softmax layer를 추가할 필요는 없습니다.\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader) # 전체 데이터를 batch만큼 나눈 값\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device) # shape: Batchx1x28x28\n",
    "        labels = labels.to(device) # shape: Batch\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1) # 10개 class중 점수가 가장 큰 값이 예측한 class입니다.\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 셋에서 이미지 한 장을 시각화해보고 모델이 올바르게 추론하는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize image and model inference\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "ridx = torch.randint(0, batch_size, (1,)) # 0~50 인덱스 중 하나를 선택해 출력해보겠습니다.\n",
    "\n",
    "image = images[ridx[0].data]\n",
    "label = labels[ridx[0].data]\n",
    "\n",
    "output = model(image.unsqueeze(0).to(device))\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "\n",
    "plt.title('test image label: {}, model inference: {}'.format(label.data, predicted[0].data), fontsize=15)\n",
    "plt.imshow(image.view(28, 28), cmap=plt.cm.Greys)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Architecture\n",
    "\n",
    "이외에도 좋은 성능을 내는 모델들은 다음과 같은 방법들을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1x1 Convolution layer (convolution with kernel_size = 1)\n",
    "\n",
    "![https://cheong.netlify.app/static/fb817ed940cd331991e5f40effdaf455/799d3/image16.png](https://cheong.netlify.app/static/fb817ed940cd331991e5f40effdaf455/799d3/image16.png)\n",
    "\n",
    "일반적으로 input의 channels을 줄여 연산량을 줄이거나, 다른 features의 channels와 개수를 일치시키고 싶을때 사용됩니다. <br/>\n",
    "예를 들어 3x3 conv, 5x5 conv와 같이 연산량이 큰 컨볼루션을 적용하기 전에 1x1 conv로 입력 차원을 줄인뒤 3x3 conv를 적용하면 더 적은 비용으로 비슷한 효과를 볼 수 있습니다. (ResNet의 Bottleneck block) <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 4, 4])\n",
      "torch.Size([1, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.ones((1, 10, 4, 4)) # C=10\n",
    "\n",
    "conv1x1 = nn.Conv2d(in_channels=10, out_channels=3, kernel_size=1, stride=1, padding=0) # kernel_size = 1\n",
    "\n",
    "out = conv1x1(inp)\n",
    "\n",
    "print(inp.shape)\n",
    "print(out.shape) # C=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Element-wise addition / Concatenate\n",
    "\n",
    "|![](https://codeforwin.org/ezoimgfmt/secureservercdn.net/160.153.138.219/b79.d22.myftpupload.com/wp-content/uploads/2015/07/matrix-addition.png?ezimgfmt=rs:392x204/rscb1)|\n",
    "|:--:|\n",
    "|Element-wise Addition|\n",
    "\n",
    "|![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile4.uf.tistory.com%2Fimage%2F99D452405B49B4DA10949D)|\n",
    "|:--:|\n",
    "|Concatenate|\n",
    "\n",
    "element-wise addition은 말 그대로 동일한 차원을 가진 tensor의 요소끼리 더하는 연산을 의미합니다. <br/>\n",
    "concatenate은 선택한 차원을 기준으로 tensors를 연결하는 것을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[9, 8, 7],\n",
      "        [6, 5, 4],\n",
      "        [3, 2, 1]])\n",
      "tensor([[10, 10, 10],\n",
      "        [10, 10, 10],\n",
      "        [10, 10, 10]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9],\n",
      "        [9, 8, 7],\n",
      "        [6, 5, 4],\n",
      "        [3, 2, 1]])\n",
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(1, 10).view(3, 3)\n",
    "B = torch.arange(9, 0, -1).view(3, 3)\n",
    "\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "out1 = A + B # element-wise addition\n",
    "print(out1)\n",
    "\n",
    "out2 = torch.cat((A, B), 0) # concat in 0 dim\n",
    "print(out2)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip connection (shortcut connection)\n",
    "\n",
    "|![](https://datascienceschool.net/upfiles/6182312059774a81a2a26246bd4e83f2.png)|\n",
    "|:---:|\n",
    "|*Skip Connection (ResNet)*|\n",
    "\n",
    "|![](https://cheong.netlify.app/static/5a711b3b3b3d4789e4d0e0fc742c0e11/7f576/image24.png)|\n",
    "|:---:|\n",
    "|*Bottleneck Block (ResNet)*|\n",
    "<br/>\n",
    "\n",
    "skip connection은 ResNet에서 제안되어졌으며 몇가지 장점들로 인해 많은 CNNs에서 필수적인 요소로 사용하고 있습니다. <br/>\n",
    "\n",
    "- 깊은 모델의 Degradation 현상 해소\n",
    "- 원활한 gradient 전파\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 28, 28])\n",
      "torch.Size([1, 256, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x_in = torch.ones((1, 256, 28, 28))\n",
    "\n",
    "conv1 = nn.Conv2d(256, 64, kernel_size=1, stride=1, padding=0) # 1x1 conv\n",
    "conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1) # 3x3 conv with padding\n",
    "conv3 = nn.Conv2d(64, 256, kernel_size=1, stride=1, padding=0) # 1x1 conv\n",
    "F = nn.Sequential(conv1, conv2, conv3) # Bottleneck block\n",
    "\n",
    "F_out = F(x_in)\n",
    "\n",
    "print(F_out.shape)\n",
    "\n",
    "x_out = F_out + x_in # skip connection (element-wise addition)\n",
    "print(x_out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt15",
   "language": "python",
   "name": "pt15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
